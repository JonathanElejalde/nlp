{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"reformer_chatbot.ipynb","provenance":[],"mount_file_id":"1SLw6GE-0jdtZMDEKXjC7nr_mIIEmnMZp","authorship_tag":"ABX9TyMjx0/VJ+i/tDzWRZ6Pw3Kf"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2I_z79B2FPil","executionInfo":{"status":"ok","timestamp":1608232373025,"user_tz":300,"elapsed":14517,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"b6aaca98-dba3-4b3a-810d-adb52bc94a37"},"source":["%cd /content/drive/MyDrive/Colab Notebooks/nlp/apps/chatbot\r\n","!pip install -q -U trax"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/nlp/apps/chatbot\n","\u001b[K     |████████████████████████████████| 471kB 5.4MB/s \n","\u001b[K     |████████████████████████████████| 3.4MB 10.8MB/s \n","\u001b[K     |████████████████████████████████| 174kB 20.8MB/s \n","\u001b[K     |████████████████████████████████| 1.5MB 25.4MB/s \n","\u001b[K     |████████████████████████████████| 348kB 32.2MB/s \n","\u001b[K     |████████████████████████████████| 1.1MB 31.8MB/s \n","\u001b[K     |████████████████████████████████| 71kB 7.9MB/s \n","\u001b[K     |████████████████████████████████| 3.7MB 33.7MB/s \n","\u001b[K     |████████████████████████████████| 890kB 35.8MB/s \n","\u001b[K     |████████████████████████████████| 2.9MB 35.4MB/s \n","\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MSR99xloHlIR"},"source":["import json\r\n","import random\r\n","import numpy as np\r\n","import trax   "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pr-PqcNJDPVl"},"source":["# Dataset\r\n","\r\n","Get the [MultiWoz dataset](https://github.com/budzianowski/multiwoz/tree/master/data)"]},{"cell_type":"markdown","metadata":{"id":"6lmwRpOuH1Hl"},"source":["# Paths"]},{"cell_type":"code","metadata":{"id":"XIBL2_XNIPEr","executionInfo":{"status":"ok","timestamp":1608258732502,"user_tz":300,"elapsed":582,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["data = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/chatbot/data/MultiWOZ_2.1'\r\n","subwords = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/chatbot/data/en_32k.subword'\r\n","output_dir = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/chatbot/models/reformer/'"],"execution_count":52,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ua_BtgSpQNN8","executionInfo":{"status":"ok","timestamp":1608237883877,"user_tz":300,"elapsed":15153,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def load_json(directory, file):\r\n","    with open(f'{directory}/{file}') as f:\r\n","        db = json.load(f)\r\n","    return db\r\n","\r\n","# Load the dialogue dataset\r\n","dialogue = load_json(data, 'data.json')"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWQdNJJ-cmDc","executionInfo":{"status":"ok","timestamp":1608238129410,"user_tz":300,"elapsed":763,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"54c2ce78-c20a-413a-b3f9-7d7e69f2326a"},"source":["dialogue_keys = list(dialogue.keys())\r\n","print(f'The amount of dialogues is: {len(dialogue_keys)}')\r\n","print(f'These are some of the dialogue keys: {dialogue_keys[10:20]}')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["The amount of dialogues is: 10438\n","These are some of the dialogue keys: ['PMUL1170.json', 'SNG01741.json', 'PMUL4899.json', 'MUL2261.json', 'SSNG0348.json', 'MUL0784.json', 'MUL0886.json', 'PMUL2512.json', 'SNG0548.json', 'MUL1474.json']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N1dV7IWTcxAp"},"source":["As we can see the dataset is composed of multiple files and the filenames are used as keys in our dictionary. Those with multi-domain dialogues have \"MUL\" in their filenames while single domain dialogues have either \"SNG\" or \"WOZ\"."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65ecycL-d3H-","executionInfo":{"status":"ok","timestamp":1608238587939,"user_tz":300,"elapsed":577,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"88388ac1-0c6b-47d3-f7a9-e634a954e53b"},"source":["# Get the keys of a file\r\n","n = 15\r\n","print(dialogue[dialogue_keys[n]].keys())"],"execution_count":14,"outputs":[{"output_type":"stream","text":["dict_keys(['goal', 'log'])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"60h0YBFYd3UT"},"source":["Each file is a dictionary with 2 keys. The `goal` also points to a dictionary and it contains several keys pertaining to the objectives of the conversation."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U4rzZ8AHezjT","executionInfo":{"status":"ok","timestamp":1608238589301,"user_tz":300,"elapsed":569,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"f7b97f2d-cf68-4751-b806-582a522884b2"},"source":["# print goal\r\n","from pprint import pprint\r\n","pprint(dialogue[dialogue_keys[n]]['goal'])"],"execution_count":15,"outputs":[{"output_type":"stream","text":["{'attraction': {},\n"," 'hospital': {},\n"," 'hotel': {'fail_info': {'area': 'north',\n","                         'parking': 'yes',\n","                         'pricerange': 'cheap',\n","                         'type': 'hotel'},\n","           'info': {'area': 'north',\n","                    'parking': 'yes',\n","                    'pricerange': 'cheap',\n","                    'type': 'guesthouse'},\n","           'reqt': ['postcode', 'internet']},\n"," 'message': ['You are planning your trip in Cambridge',\n","             \"You are looking for a <span class='emphasis'>train</span>. The \"\n","             \"train should <span class='emphasis'>arrive by 08:15</span> and \"\n","             \"should go to <span class='emphasis'>cambridge</span>\",\n","             \"The train should leave on <span class='emphasis'>monday</span> \"\n","             \"and should depart from <span class='emphasis'>bishops \"\n","             'stortford</span>',\n","             'Once you find the train you want to make a booking for <span '\n","             \"class='emphasis'>2 people</span>\",\n","             \"Make sure you get the <span class='emphasis'>reference \"\n","             'number</span>',\n","             \"You are also looking for a <span class='emphasis'>place to \"\n","             'stay</span>. The hotel should be in the type of <span '\n","             \"class='emphasis'>hotel</span> and should be in the <span \"\n","             \"class='emphasis'>north</span>\",\n","             \"The hotel should be in the <span class='emphasis'>cheap</span> \"\n","             \"price range and should <span class='emphasis'>include free \"\n","             'parking</span>',\n","             'If there is no such hotel, how about one that is in <span '\n","             \"class='emphasis'>the type of guesthouse</span>\",\n","             \"Make sure you get <span class='emphasis'>postcode</span> and \"\n","             \"<span class='emphasis'>whether they have internet</span>\"],\n"," 'police': {},\n"," 'restaurant': {},\n"," 'taxi': {},\n"," 'train': {'book': {'invalid': True, 'people': '2'},\n","           'fail_book': {},\n","           'fail_info': {},\n","           'info': {'arriveBy': '08:15',\n","                    'day': 'monday',\n","                    'departure': 'bishops stortford',\n","                    'destination': 'cambridge'}}}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VqpkBqMUe-1c"},"source":["The `log` on the other hand contains the dialog. It is a list of dictionaries and each element of this list contains several descriptions as well. Let's look at an example:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0z5AQRhufjTT","executionInfo":{"status":"ok","timestamp":1608238688610,"user_tz":300,"elapsed":561,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"5ee881e7-c30c-41a5-8e49-74c3c7254917"},"source":["# get first element of the log list\r\n","dialogue[dialogue_keys[n]]['log'][0]"],"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'dialog_act': {'Train-Inform': [['Dest', 'cambridge'], ['Arrive', '08:15']]},\n"," 'metadata': {},\n"," 'span_info': [['Train-Inform', 'Dest', 'cambridge', 10, 10],\n","  ['Train-Inform', 'Arrive', '08:15', 12, 12]],\n"," 'text': 'Hi I am looking for a train to arrive in Cambridge by 08:15.'}"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"nh7u3bAFfsty"},"source":["We are only interested in the conversation which is in the `text` field.\r\n","The conversation goes back and forth between two persons. Let's call them 'Person 1' and 'Person 2'. This implies that\r\n","```\r\n","data['SNG0073.json']['log'][0]['text']\r\n","``` is 'Person 1' and\r\n","```data['SNG0073.json']['log'][1]['text']``` is 'Person 2' and so on. The even offsets are 'Person 1' and the odd offsets are 'Person 2'."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X9meRtV2gIqo","executionInfo":{"status":"ok","timestamp":1608238925395,"user_tz":300,"elapsed":519,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"123a40c9-db00-495f-c9c6-fada3da5a08c"},"source":["print(' Person 1: ', dialogue[dialogue_keys[n]]['log'][0]['text'])\r\n","print(' Person 2: ',dialogue[dialogue_keys[n]]['log'][1]['text'])"],"execution_count":20,"outputs":[{"output_type":"stream","text":[" Person 1:  Hi I am looking for a train to arrive in Cambridge by 08:15.\n"," Person 2:  Certainly, where will you be departing from?\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hnDbXU5Vgk1Z"},"source":["# Extract conversations"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pzU6U3oygxkP","executionInfo":{"status":"ok","timestamp":1608239835653,"user_tz":300,"elapsed":546,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"90b12b5e-ec1e-4249-9c69-629f0600798e"},"source":["def get_conversation(dataset, filename):\r\n","    \"\"\"\r\n","    Takes the dialogue dataset and extracts the\r\n","    dialogues for each log\r\n","    Args:\r\n","        dataset: dict\r\n","        filename:str\r\n","    returns:\r\n","        result: str\r\n","    \"\"\"\r\n","    result = ''\r\n","\r\n","    # Get length of file's log list\r\n","    message_len = len(dataset[filename]['log'])\r\n","\r\n","    # Set delimiter strings for each person in the dialogue\r\n","    delimiter1 = ' Person 1: '\r\n","    delimiter2 = ' Person 2: '\r\n","\r\n","    for i in range(message_len):\r\n","        current_log = dataset[filename]['log'][i]\r\n","\r\n","        # check person, if even = person1\r\n","        if i % 2 == 0:\r\n","            result += delimiter1\r\n","        else:\r\n","            result += delimiter2\r\n","\r\n","        # append message text from the log\r\n","        result += current_log['text']\r\n","\r\n","    return result\r\n","\r\n","# Uncomment for testing\r\n","\r\n","# # test\r\n","# n_file = 50\r\n","# filename = dialogue_keys[n_file]\r\n","# result_dialogue_test = get_conversation(dialogue, filename)\r\n","# print(result_dialogue_test)"],"execution_count":26,"outputs":[{"output_type":"stream","text":[" Person 1: I need a restaurant to dine at in Cambridge on my upcoming trip. Person 2: There are lots to choose from. What type of cuisine are you looking for? Person 1: I don't care. It needs to be on the south side and moderately priced.  Person 2: There are 2 options, pizza hut cherry hinton which serves italian and restaurant alimentum which serves modern european.  Can I book you for those? Person 1: Yes please.  I also need a hotel with at least 3 stars and free parking. Person 2: There is no hotel in south side,do you want me to try different area? Person 1: How about a Guesthouse in South side instead? Person 2: I'm sorry, there is no guest house that meets those criteria, either. Would you like to try a different rating, or a different area? Person 1: Sure, what about in the city centre? Person 2: I am not finding a guesthouse that meets your criteria.  Might a hotel work? Person 1: Is there not one in the same area as the restaurant? Person 2: There are 2, the Alexander Bed and Breakfast 4 star or the El Shaddai 0 star.  Both are in the cheap range and in the Centre.  Person 1: Okay. Can you try booking the El Shaddai for me fo Saturday for 2 people for 3 nights. Person 2: The booking was successful.  Your reference number is S8LZAB9P.  Is there anything else I can help with today? Person 1: No that would be everything, thank you! Person 2: Okay! Glad I could help. Enjoy your stay. Person 1: Thank you, I will. Person 2: Good bye, thank you. \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0PhG56rQjQ2N"},"source":["# Process the conversations for the reformer inputs "]},{"cell_type":"code","metadata":{"id":"UGhiDiuMHEvv","executionInfo":{"status":"ok","timestamp":1608249365816,"user_tz":300,"elapsed":542,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def get_all_conversations(dataset, files):\r\n","    \"\"\"\r\n","    Takes the dialogue dataset and gets all the conversations\r\n","    available. Then returns each conversation in a list of\r\n","    strings\r\n","    Args:\r\n","        dataset: dict\r\n","        files: list\r\n","    returns:\r\n","        all_conversations: list\r\n","    \"\"\"\r\n","    all_conversations = []\r\n","\r\n","    for filename in files:\r\n","        conversation = get_conversation(dataset, filename)\r\n","        all_conversations.append(conversation)\r\n","\r\n","    return all_conversations"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Xn_DHEuHqTp","executionInfo":{"status":"ok","timestamp":1608249408488,"user_tz":300,"elapsed":2534,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["conversations = get_all_conversations(dialogue, dialogue_keys)"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sD2uiNMuInGt"},"source":["# Split data into train/test"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NHcyZ-CDIojj","executionInfo":{"status":"ok","timestamp":1608249661669,"user_tz":300,"elapsed":586,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"0c20099f-4dc2-4863-f579-7c7eb50ecc34"},"source":["random.shuffle(conversations)\r\n","train_split = int(len(conversations) * 0.95)\r\n","train = conversations[:train_split]\r\n","test = conversations[train_split:]\r\n","\r\n","print(f'number of conversations in the data set: {len(conversations)}')\r\n","print(f'number of conversations in train set: {len(train)}')\r\n","print(f'number of conversations in test set: {len(test)}')\r\n","\r\n"],"execution_count":33,"outputs":[{"output_type":"stream","text":["number of conversations in the data set: 10438\n","number of conversations in train set: 9916\n","number of conversations in test set: 522\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6t-eHK2aJXZs"},"source":["# Tokenize data\r\n","\r\n","First we will define a utility generator function to yield elements from our dataset. Then, we will define our data pipeline for tokenizing and batching our data. We will bucket by length and also have an upper bound on the token length."]},{"cell_type":"code","metadata":{"id":"HPh9ZFF5KG3w","executionInfo":{"status":"ok","timestamp":1608251044320,"user_tz":300,"elapsed":515,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def stream(data):\r\n","    while True:\r\n","        conversation = random.choice(data)\r\n","\r\n","        yield (conversation, conversation)\r\n","\r\n","data_pipeline = trax.data.Serial(\r\n","    # Randomize the stream\r\n","    trax.data.Shuffle(),\r\n","\r\n","    # Tokenize the data\r\n","    trax.data.Tokenize(vocab_file=subwords),\r\n","\r\n","    # Filter long sequences\r\n","    trax.data.FilterByLength(2048),\r\n","\r\n","    # Bucket by length\r\n","    trax.data.BucketByLength(boundaries=[128, 256, 512, 1024],\r\n","                             batch_sizes=[16, 8, 4, 2, 1]),\r\n","    \r\n","    # Add loss weights but do not add it to the padding tokens\r\n","    trax.data.AddLossWeights(id_to_mask=0)\r\n",")\r\n","\r\n","# Apply the data pipeline to our train and eval sets\r\n","train_stream = data_pipeline(stream(train))\r\n","eval_stream = data_pipeline(stream(test))\r\n"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"T6Gb9isMNEDM","executionInfo":{"status":"ok","timestamp":1608252639161,"user_tz":300,"elapsed":701,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"1e1ab840-0934-42d0-d2ca-65a2251dcb81"},"source":["# Avoiding scrolling bars\r\n","from IPython.display import HTML\r\n","display(HTML('''\r\n","<style>\r\n","  pre {\r\n","      white-space: normal;\r\n","  }\r\n","</style>\r\n","'''))\r\n","\r\n","# Uncomment for test\r\n","\r\n","# # The stream generators will yield (input, target, weights). let's just grab the input for inspection\r\n","# inp, _, _ = next(train_stream)\r\n","\r\n","# # Print the shape. format is (batch size, token length)\r\n","# print(\"input shape: \", inp.shape)\r\n","\r\n","# # Detokenize the first element\r\n","# print(trax.data.detokenize(inp[0], vocab_file=subwords))"],"execution_count":49,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","<style>\n","  pre {\n","      white-space: normal;\n","  }\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"sf-iLrLTPQvb"},"source":["# Reformer language model"]},{"cell_type":"code","metadata":{"id":"y-zDgCI8nxnW","executionInfo":{"status":"ok","timestamp":1608258263843,"user_tz":300,"elapsed":692,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def Reformer_language_model(vocab_size=33000, n_layers=2, mode='train', attention_type=trax.layers.SelfAttention):\r\n","    \"\"\"\r\n","    Implements a wrapper that returns a Reformer Language Model\r\n","    Args:\r\n","        vocab_size: int\r\n","        n_layers: int. number of decoder layers\r\n","        mode: str\r\n","        attention_type: class. an attention class to use\r\n","    returns:\r\n","        model: ReformerLM implemented in trax\r\n","    \"\"\"\r\n","    model = trax.models.ReformerLM(vocab_size=vocab_size, n_layers=n_layers, \r\n","                                   mode=mode, attention_type=attention_type)\r\n","    \r\n","    return model\r\n"],"execution_count":50,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PsFe6iVhqZbR"},"source":["# Training loop"]},{"cell_type":"code","metadata":{"id":"VGfZPrlEqgYf","executionInfo":{"status":"ok","timestamp":1608258669223,"user_tz":300,"elapsed":493,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def training_loop(Reformer, train_gen, test_gen, learning_rate=0.01, output_dir='./model/'):\r\n","\r\n","    # Use the warmup_and_rsqrt_decay learning rate schedule\r\n","    lr_schedule = trax.lr.warmup_and_rsqrt_decay(\r\n","        n_warmup_steps=1000, max_value=0.01)\r\n","    \r\n","    train_task = trax.supervised.TrainTask(\r\n","        train_gen,\r\n","        trax.layers.CrossEntropyLoss(),\r\n","        trax.optimizers.Adam(learning_rate),\r\n","        lr_schedule,\r\n","        n_steps_per_checkpoint=50\r\n","    )\r\n","\r\n","    eval_task = trax.supervised.EvalTask(\r\n","        eval_gen,\r\n","        metrics=[trax.layers.CrossEntropyLoss(), trax.layers.Accuracy()]\r\n","    )\r\n","\r\n","    loop = trax.supervised.training.Loop(\r\n","        Reformer(mode='train'),\r\n","        train_task,\r\n","        eval_task=[eval_task],\r\n","        output_dir=output_dir\r\n","    )\r\n","\r\n","    return loop\r\n"],"execution_count":51,"outputs":[]},{"cell_type":"code","metadata":{"id":"S_8yhgbbr8hk"},"source":[""],"execution_count":null,"outputs":[]}]}