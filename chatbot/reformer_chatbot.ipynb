{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"reformer_chatbot.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1SLw6GE-0jdtZMDEKXjC7nr_mIIEmnMZp","authorship_tag":"ABX9TyOMxMIfqGNHA4ILS4L8A+76"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2I_z79B2FPil","executionInfo":{"status":"ok","timestamp":1608259223130,"user_tz":300,"elapsed":3007,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"e4733b02-ab43-45f0-b83b-927f8b760a28"},"source":["%cd /content/drive/MyDrive/Colab Notebooks/nlp/apps/chatbot\r\n","!pip install -q -U trax"],"execution_count":20,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/nlp/apps/chatbot\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MSR99xloHlIR","executionInfo":{"status":"ok","timestamp":1608259223132,"user_tz":300,"elapsed":2999,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["import json\r\n","import random\r\n","import numpy as np\r\n","import trax   "],"execution_count":21,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pr-PqcNJDPVl"},"source":["# Dataset\r\n","\r\n","Get the [MultiWoz dataset](https://github.com/budzianowski/multiwoz/tree/master/data)"]},{"cell_type":"markdown","metadata":{"id":"6lmwRpOuH1Hl"},"source":["# Paths"]},{"cell_type":"code","metadata":{"id":"XIBL2_XNIPEr","executionInfo":{"status":"ok","timestamp":1608259223134,"user_tz":300,"elapsed":2995,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["data = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/chatbot/data/MultiWOZ_2.1'\r\n","subwords = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/chatbot/data/en_32k.subword'\r\n","output_dir = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/chatbot/models/reformer/'"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ua_BtgSpQNN8","executionInfo":{"status":"ok","timestamp":1608259229505,"user_tz":300,"elapsed":9363,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def load_json(directory, file):\r\n","    with open(f'{directory}/{file}') as f:\r\n","        db = json.load(f)\r\n","    return db\r\n","\r\n","# Load the dialogue dataset\r\n","dialogue = load_json(data, 'data.json')"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWQdNJJ-cmDc","executionInfo":{"status":"ok","timestamp":1608259229519,"user_tz":300,"elapsed":9370,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"782f3a45-3e00-48bb-cd8f-4c8e1583b7a6"},"source":["dialogue_keys = list(dialogue.keys())\r\n","print(f'The amount of dialogues is: {len(dialogue_keys)}')\r\n","print(f'These are some of the dialogue keys: {dialogue_keys[10:20]}')"],"execution_count":24,"outputs":[{"output_type":"stream","text":["The amount of dialogues is: 10438\n","These are some of the dialogue keys: ['PMUL1170.json', 'SNG01741.json', 'PMUL4899.json', 'MUL2261.json', 'SSNG0348.json', 'MUL0784.json', 'MUL0886.json', 'PMUL2512.json', 'SNG0548.json', 'MUL1474.json']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"N1dV7IWTcxAp"},"source":["As we can see the dataset is composed of multiple files and the filenames are used as keys in our dictionary. Those with multi-domain dialogues have \"MUL\" in their filenames while single domain dialogues have either \"SNG\" or \"WOZ\"."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"65ecycL-d3H-","executionInfo":{"status":"ok","timestamp":1608259229521,"user_tz":300,"elapsed":9364,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"674d8222-9aeb-4f37-e563-08492c2c53f6"},"source":["# Get the keys of a file\r\n","n = 15\r\n","print(dialogue[dialogue_keys[n]].keys())"],"execution_count":25,"outputs":[{"output_type":"stream","text":["dict_keys(['goal', 'log'])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"60h0YBFYd3UT"},"source":["Each file is a dictionary with 2 keys. The `goal` also points to a dictionary and it contains several keys pertaining to the objectives of the conversation."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U4rzZ8AHezjT","executionInfo":{"status":"ok","timestamp":1608259229523,"user_tz":300,"elapsed":9360,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"4a71e7af-5830-45ad-e908-fec262e51720"},"source":["# print goal\r\n","from pprint import pprint\r\n","pprint(dialogue[dialogue_keys[n]]['goal'])"],"execution_count":26,"outputs":[{"output_type":"stream","text":["{'attraction': {},\n"," 'hospital': {},\n"," 'hotel': {'fail_info': {'area': 'north',\n","                         'parking': 'yes',\n","                         'pricerange': 'cheap',\n","                         'type': 'hotel'},\n","           'info': {'area': 'north',\n","                    'parking': 'yes',\n","                    'pricerange': 'cheap',\n","                    'type': 'guesthouse'},\n","           'reqt': ['postcode', 'internet']},\n"," 'message': ['You are planning your trip in Cambridge',\n","             \"You are looking for a <span class='emphasis'>train</span>. The \"\n","             \"train should <span class='emphasis'>arrive by 08:15</span> and \"\n","             \"should go to <span class='emphasis'>cambridge</span>\",\n","             \"The train should leave on <span class='emphasis'>monday</span> \"\n","             \"and should depart from <span class='emphasis'>bishops \"\n","             'stortford</span>',\n","             'Once you find the train you want to make a booking for <span '\n","             \"class='emphasis'>2 people</span>\",\n","             \"Make sure you get the <span class='emphasis'>reference \"\n","             'number</span>',\n","             \"You are also looking for a <span class='emphasis'>place to \"\n","             'stay</span>. The hotel should be in the type of <span '\n","             \"class='emphasis'>hotel</span> and should be in the <span \"\n","             \"class='emphasis'>north</span>\",\n","             \"The hotel should be in the <span class='emphasis'>cheap</span> \"\n","             \"price range and should <span class='emphasis'>include free \"\n","             'parking</span>',\n","             'If there is no such hotel, how about one that is in <span '\n","             \"class='emphasis'>the type of guesthouse</span>\",\n","             \"Make sure you get <span class='emphasis'>postcode</span> and \"\n","             \"<span class='emphasis'>whether they have internet</span>\"],\n"," 'police': {},\n"," 'restaurant': {},\n"," 'taxi': {},\n"," 'train': {'book': {'invalid': True, 'people': '2'},\n","           'fail_book': {},\n","           'fail_info': {},\n","           'info': {'arriveBy': '08:15',\n","                    'day': 'monday',\n","                    'departure': 'bishops stortford',\n","                    'destination': 'cambridge'}}}\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VqpkBqMUe-1c"},"source":["The `log` on the other hand contains the dialog. It is a list of dictionaries and each element of this list contains several descriptions as well. Let's look at an example:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0z5AQRhufjTT","executionInfo":{"status":"ok","timestamp":1608259229525,"user_tz":300,"elapsed":9355,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"0ac0cc53-a69f-426e-8106-835d79a9a98b"},"source":["# get first element of the log list\r\n","dialogue[dialogue_keys[n]]['log'][0]"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'dialog_act': {'Train-Inform': [['Dest', 'cambridge'], ['Arrive', '08:15']]},\n"," 'metadata': {},\n"," 'span_info': [['Train-Inform', 'Dest', 'cambridge', 10, 10],\n","  ['Train-Inform', 'Arrive', '08:15', 12, 12]],\n"," 'text': 'Hi I am looking for a train to arrive in Cambridge by 08:15.'}"]},"metadata":{"tags":[]},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"nh7u3bAFfsty"},"source":["We are only interested in the conversation which is in the `text` field.\r\n","The conversation goes back and forth between two persons. Let's call them 'Person 1' and 'Person 2'. This implies that\r\n","```\r\n","data['SNG0073.json']['log'][0]['text']\r\n","``` is 'Person 1' and\r\n","```data['SNG0073.json']['log'][1]['text']``` is 'Person 2' and so on. The even offsets are 'Person 1' and the odd offsets are 'Person 2'."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X9meRtV2gIqo","executionInfo":{"status":"ok","timestamp":1608259229527,"user_tz":300,"elapsed":9350,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"d5156a88-a007-4d16-c60a-99a9d1a179bd"},"source":["print(' Person 1: ', dialogue[dialogue_keys[n]]['log'][0]['text'])\r\n","print(' Person 2: ',dialogue[dialogue_keys[n]]['log'][1]['text'])"],"execution_count":28,"outputs":[{"output_type":"stream","text":[" Person 1:  Hi I am looking for a train to arrive in Cambridge by 08:15.\n"," Person 2:  Certainly, where will you be departing from?\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hnDbXU5Vgk1Z"},"source":["# Extract conversations"]},{"cell_type":"code","metadata":{"id":"pzU6U3oygxkP","executionInfo":{"status":"ok","timestamp":1608259229528,"user_tz":300,"elapsed":9344,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def get_conversation(dataset, filename):\r\n","    \"\"\"\r\n","    Takes the dialogue dataset and extracts the\r\n","    dialogues for each log\r\n","    Args:\r\n","        dataset: dict\r\n","        filename:str\r\n","    returns:\r\n","        result: str\r\n","    \"\"\"\r\n","    result = ''\r\n","\r\n","    # Get length of file's log list\r\n","    message_len = len(dataset[filename]['log'])\r\n","\r\n","    # Set delimiter strings for each person in the dialogue\r\n","    delimiter1 = ' Person 1: '\r\n","    delimiter2 = ' Person 2: '\r\n","\r\n","    for i in range(message_len):\r\n","        current_log = dataset[filename]['log'][i]\r\n","\r\n","        # check person, if even = person1\r\n","        if i % 2 == 0:\r\n","            result += delimiter1\r\n","        else:\r\n","            result += delimiter2\r\n","\r\n","        # append message text from the log\r\n","        result += current_log['text']\r\n","\r\n","    return result\r\n","\r\n","# Uncomment for testing\r\n","\r\n","# # test\r\n","# n_file = 50\r\n","# filename = dialogue_keys[n_file]\r\n","# result_dialogue_test = get_conversation(dialogue, filename)\r\n","# print(result_dialogue_test)"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0PhG56rQjQ2N"},"source":["# Process the conversations for the reformer inputs "]},{"cell_type":"code","metadata":{"id":"UGhiDiuMHEvv","executionInfo":{"status":"ok","timestamp":1608259229530,"user_tz":300,"elapsed":9342,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def get_all_conversations(dataset, files):\r\n","    \"\"\"\r\n","    Takes the dialogue dataset and gets all the conversations\r\n","    available. Then returns each conversation in a list of\r\n","    strings\r\n","    Args:\r\n","        dataset: dict\r\n","        files: list\r\n","    returns:\r\n","        all_conversations: list\r\n","    \"\"\"\r\n","    all_conversations = []\r\n","\r\n","    for filename in files:\r\n","        conversation = get_conversation(dataset, filename)\r\n","        all_conversations.append(conversation)\r\n","\r\n","    return all_conversations"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Xn_DHEuHqTp","executionInfo":{"status":"ok","timestamp":1608259229802,"user_tz":300,"elapsed":9611,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["conversations = get_all_conversations(dialogue, dialogue_keys)"],"execution_count":31,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sD2uiNMuInGt"},"source":["# Split data into train/test"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NHcyZ-CDIojj","executionInfo":{"status":"ok","timestamp":1608259229804,"user_tz":300,"elapsed":9608,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"3094e457-7188-48b9-ab09-f72c43e93615"},"source":["random.shuffle(conversations)\r\n","train_split = int(len(conversations) * 0.95)\r\n","train = conversations[:train_split]\r\n","test = conversations[train_split:]\r\n","\r\n","print(f'number of conversations in the data set: {len(conversations)}')\r\n","print(f'number of conversations in train set: {len(train)}')\r\n","print(f'number of conversations in test set: {len(test)}')\r\n","\r\n"],"execution_count":32,"outputs":[{"output_type":"stream","text":["number of conversations in the data set: 10438\n","number of conversations in train set: 9916\n","number of conversations in test set: 522\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6t-eHK2aJXZs"},"source":["# Tokenize data\r\n","\r\n","First we will define a utility generator function to yield elements from our dataset. Then, we will define our data pipeline for tokenizing and batching our data. We will bucket by length and also have an upper bound on the token length."]},{"cell_type":"code","metadata":{"id":"HPh9ZFF5KG3w","executionInfo":{"status":"ok","timestamp":1608259229807,"user_tz":300,"elapsed":9606,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def stream(data):\r\n","    while True:\r\n","        conversation = random.choice(data)\r\n","\r\n","        yield (conversation, conversation)\r\n","\r\n","data_pipeline = trax.data.Serial(\r\n","    # Randomize the stream\r\n","    trax.data.Shuffle(),\r\n","\r\n","    # Tokenize the data\r\n","    trax.data.Tokenize(vocab_file=subwords),\r\n","\r\n","    # Filter long sequences\r\n","    trax.data.FilterByLength(2048),\r\n","\r\n","    # Bucket by length\r\n","    trax.data.BucketByLength(boundaries=[128, 256, 512, 1024],\r\n","                             batch_sizes=[16, 8, 4, 2, 1]),\r\n","    \r\n","    # Add loss weights but do not add it to the padding tokens\r\n","    trax.data.AddLossWeights(id_to_mask=0)\r\n",")\r\n","\r\n","# Apply the data pipeline to our train and eval sets\r\n","train_stream = data_pipeline(stream(train))\r\n","eval_stream = data_pipeline(stream(test))\r\n"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":17},"id":"T6Gb9isMNEDM","executionInfo":{"status":"ok","timestamp":1608259229809,"user_tz":300,"elapsed":9599,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"247e1366-cf82-4c80-cfaa-57a2c1d2b4e9"},"source":["# Avoiding scrolling bars\r\n","from IPython.display import HTML\r\n","display(HTML('''\r\n","<style>\r\n","  pre {\r\n","      white-space: normal;\r\n","  }\r\n","</style>\r\n","'''))\r\n","\r\n","# Uncomment for test\r\n","\r\n","# # The stream generators will yield (input, target, weights). let's just grab the input for inspection\r\n","# inp, _, _ = next(train_stream)\r\n","\r\n","# # Print the shape. format is (batch size, token length)\r\n","# print(\"input shape: \", inp.shape)\r\n","\r\n","# # Detokenize the first element\r\n","# print(trax.data.detokenize(inp[0], vocab_file=subwords))"],"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","<style>\n","  pre {\n","      white-space: normal;\n","  }\n","</style>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"sf-iLrLTPQvb"},"source":["# Reformer language model"]},{"cell_type":"code","metadata":{"id":"y-zDgCI8nxnW","executionInfo":{"status":"ok","timestamp":1608259229810,"user_tz":300,"elapsed":9594,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def reformer_language_model(vocab_size=33000, n_layers=2, mode='train', attention_type=trax.layers.SelfAttention):\r\n","    \"\"\"\r\n","    Implements a wrapper that returns a Reformer Language Model\r\n","    Args:\r\n","        vocab_size: int\r\n","        n_layers: int. number of decoder layers\r\n","        mode: str\r\n","        attention_type: class. an attention class to use\r\n","    returns:\r\n","        model: ReformerLM implemented in trax\r\n","    \"\"\"\r\n","    model = trax.models.ReformerLM(vocab_size=vocab_size, n_layers=n_layers, \r\n","                                   mode=mode, attention_type=attention_type)\r\n","    \r\n","    return model\r\n"],"execution_count":35,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PsFe6iVhqZbR"},"source":["# Training loop"]},{"cell_type":"code","metadata":{"id":"VGfZPrlEqgYf","executionInfo":{"status":"ok","timestamp":1608259298067,"user_tz":300,"elapsed":596,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def training_loop(Reformer, train_gen, test_gen, output_dir, learning_rate=0.01):\r\n","\r\n","    # Use the warmup_and_rsqrt_decay learning rate schedule\r\n","    lr_schedule = trax.lr.warmup_and_rsqrt_decay(\r\n","        n_warmup_steps=1000, max_value=0.01)\r\n","    \r\n","    train_task = trax.supervised.TrainTask(\r\n","        train_gen,\r\n","        trax.layers.CrossEntropyLoss(),\r\n","        trax.optimizers.Adam(learning_rate),\r\n","        lr_schedule,\r\n","        n_steps_per_checkpoint=50\r\n","    )\r\n","\r\n","    eval_task = trax.supervised.EvalTask(\r\n","        test_gen,\r\n","        metrics=[trax.layers.CrossEntropyLoss(), trax.layers.Accuracy()]\r\n","    )\r\n","\r\n","    loop = trax.supervised.training.Loop(\r\n","        Reformer(mode='train'),\r\n","        train_task,\r\n","        eval_tasks=[eval_task],\r\n","        output_dir=output_dir\r\n","    )\r\n","\r\n","    return loop\r\n"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S_8yhgbbr8hk","executionInfo":{"status":"ok","timestamp":1608266127218,"user_tz":300,"elapsed":4521695,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"0a786f96-4746-48f4-8d41-aaa426a33e3c"},"source":["# an approximation of en_steps by epoch. Because we don't have a \r\n","# fix batch_size we can really calculate how many steps a epoch might take\r\n","epochs = 10\r\n","n_steps = int(len(train) / 16) * epochs\r\n","loop = training_loop(reformer_language_model, train_stream, eval_stream, output_dir)\r\n","print(f'n_steps: {n_steps}')\r\n","loop.run(n_steps=n_steps)"],"execution_count":48,"outputs":[{"output_type":"stream","text":["n_steps: 6190\n","\n","Step   1850: Ran 50 train steps in 64.28 secs\n","Step   1850: train CrossEntropyLoss |  4.61635876\n","Step   1850: eval  CrossEntropyLoss |  4.67196035\n","Step   1850: eval          Accuracy |  0.19824088\n","\n","Step   1900: Ran 50 train steps in 36.21 secs\n","Step   1900: train CrossEntropyLoss |  4.67583561\n","Step   1900: eval  CrossEntropyLoss |  4.44042587\n","Step   1900: eval          Accuracy |  0.22091976\n","\n","Step   1950: Ran 50 train steps in 35.65 secs\n","Step   1950: train CrossEntropyLoss |  4.52507496\n","Step   1950: eval  CrossEntropyLoss |  4.43591690\n","Step   1950: eval          Accuracy |  0.20526724\n","\n","Step   2000: Ran 50 train steps in 35.94 secs\n","Step   2000: train CrossEntropyLoss |  4.50447607\n","Step   2000: eval  CrossEntropyLoss |  4.56526995\n","Step   2000: eval          Accuracy |  0.22515635\n","\n","Step   2050: Ran 50 train steps in 35.98 secs\n","Step   2050: train CrossEntropyLoss |  4.46527195\n","Step   2050: eval  CrossEntropyLoss |  4.66826296\n","Step   2050: eval          Accuracy |  0.19634990\n","\n","Step   2100: Ran 50 train steps in 35.95 secs\n","Step   2100: train CrossEntropyLoss |  4.44580030\n","Step   2100: eval  CrossEntropyLoss |  4.44706249\n","Step   2100: eval          Accuracy |  0.20860216\n","\n","Step   2150: Ran 50 train steps in 36.45 secs\n","Step   2150: train CrossEntropyLoss |  4.38195276\n","Step   2150: eval  CrossEntropyLoss |  4.50159740\n","Step   2150: eval          Accuracy |  0.23163052\n","\n","Step   2200: Ran 50 train steps in 36.29 secs\n","Step   2200: train CrossEntropyLoss |  4.35693836\n","Step   2200: eval  CrossEntropyLoss |  4.55221510\n","Step   2200: eval          Accuracy |  0.21971408\n","\n","Step   2250: Ran 50 train steps in 36.42 secs\n","Step   2250: train CrossEntropyLoss |  4.35206652\n","Step   2250: eval  CrossEntropyLoss |  4.30148172\n","Step   2250: eval          Accuracy |  0.23305671\n","\n","Step   2300: Ran 50 train steps in 36.04 secs\n","Step   2300: train CrossEntropyLoss |  4.33123255\n","Step   2300: eval  CrossEntropyLoss |  4.27707911\n","Step   2300: eval          Accuracy |  0.22437137\n","\n","Step   2350: Ran 50 train steps in 36.94 secs\n","Step   2350: train CrossEntropyLoss |  4.32307339\n","Step   2350: eval  CrossEntropyLoss |  4.07049894\n","Step   2350: eval          Accuracy |  0.24570201\n","\n","Step   2400: Ran 50 train steps in 36.51 secs\n","Step   2400: train CrossEntropyLoss |  4.28733921\n","Step   2400: eval  CrossEntropyLoss |  4.25678015\n","Step   2400: eval          Accuracy |  0.23783785\n","\n","Step   2450: Ran 50 train steps in 36.38 secs\n","Step   2450: train CrossEntropyLoss |  4.26568699\n","Step   2450: eval  CrossEntropyLoss |  4.20831776\n","Step   2450: eval          Accuracy |  0.24540779\n","\n","Step   2500: Ran 50 train steps in 41.67 secs\n","Step   2500: train CrossEntropyLoss |  4.24173403\n","Step   2500: eval  CrossEntropyLoss |  4.12476110\n","Step   2500: eval          Accuracy |  0.24656594\n","\n","Step   2550: Ran 50 train steps in 35.81 secs\n","Step   2550: train CrossEntropyLoss |  4.27610254\n","Step   2550: eval  CrossEntropyLoss |  4.23480129\n","Step   2550: eval          Accuracy |  0.25133333\n","\n","Step   2600: Ran 50 train steps in 35.82 secs\n","Step   2600: train CrossEntropyLoss |  4.22914505\n","Step   2600: eval  CrossEntropyLoss |  4.12565422\n","Step   2600: eval          Accuracy |  0.24075596\n","\n","Step   2650: Ran 50 train steps in 36.13 secs\n","Step   2650: train CrossEntropyLoss |  4.24793434\n","Step   2650: eval  CrossEntropyLoss |  4.25366974\n","Step   2650: eval          Accuracy |  0.24397165\n","\n","Step   2700: Ran 50 train steps in 36.38 secs\n","Step   2700: train CrossEntropyLoss |  4.22444248\n","Step   2700: eval  CrossEntropyLoss |  4.19551516\n","Step   2700: eval          Accuracy |  0.24508886\n","\n","Step   2750: Ran 50 train steps in 37.16 secs\n","Step   2750: train CrossEntropyLoss |  4.19704580\n","Step   2750: eval  CrossEntropyLoss |  4.18958712\n","Step   2750: eval          Accuracy |  0.23697726\n","\n","Step   2800: Ran 50 train steps in 36.59 secs\n","Step   2800: train CrossEntropyLoss |  4.18888950\n","Step   2800: eval  CrossEntropyLoss |  4.19094419\n","Step   2800: eval          Accuracy |  0.27847221\n","\n","Step   2850: Ran 50 train steps in 37.13 secs\n","Step   2850: train CrossEntropyLoss |  4.13658142\n","Step   2850: eval  CrossEntropyLoss |  4.25857925\n","Step   2850: eval          Accuracy |  0.24542125\n","\n","Step   2900: Ran 50 train steps in 36.28 secs\n","Step   2900: train CrossEntropyLoss |  4.17814922\n","Step   2900: eval  CrossEntropyLoss |  4.14158010\n","Step   2900: eval          Accuracy |  0.25287357\n","\n","Step   2950: Ran 50 train steps in 36.61 secs\n","Step   2950: train CrossEntropyLoss |  4.15469551\n","Step   2950: eval  CrossEntropyLoss |  4.26760530\n","Step   2950: eval          Accuracy |  0.23840114\n","\n","Step   3000: Ran 50 train steps in 36.54 secs\n","Step   3000: train CrossEntropyLoss |  4.14035559\n","Step   3000: eval  CrossEntropyLoss |  4.27003813\n","Step   3000: eval          Accuracy |  0.26568758\n","\n","Step   3050: Ran 50 train steps in 36.70 secs\n","Step   3050: train CrossEntropyLoss |  4.12390709\n","Step   3050: eval  CrossEntropyLoss |  4.18524504\n","Step   3050: eval          Accuracy |  0.24060151\n","\n","Step   3100: Ran 50 train steps in 37.32 secs\n","Step   3100: train CrossEntropyLoss |  4.12198210\n","Step   3100: eval  CrossEntropyLoss |  4.17592430\n","Step   3100: eval          Accuracy |  0.25850341\n","\n","Step   3150: Ran 50 train steps in 36.55 secs\n","Step   3150: train CrossEntropyLoss |  4.11584616\n","Step   3150: eval  CrossEntropyLoss |  4.31108475\n","Step   3150: eval          Accuracy |  0.23596574\n","\n","Step   3200: Ran 50 train steps in 36.01 secs\n","Step   3200: train CrossEntropyLoss |  4.11788988\n","Step   3200: eval  CrossEntropyLoss |  4.15706110\n","Step   3200: eval          Accuracy |  0.29197082\n","\n","Step   3250: Ran 50 train steps in 36.61 secs\n","Step   3250: train CrossEntropyLoss |  4.12010384\n","Step   3250: eval  CrossEntropyLoss |  4.18254137\n","Step   3250: eval          Accuracy |  0.25242719\n","\n","Step   3300: Ran 50 train steps in 43.01 secs\n","Step   3300: train CrossEntropyLoss |  4.12198734\n","Step   3300: eval  CrossEntropyLoss |  4.20521259\n","Step   3300: eval          Accuracy |  0.25597748\n","\n","Step   3350: Ran 50 train steps in 37.46 secs\n","Step   3350: train CrossEntropyLoss |  4.10250807\n","Step   3350: eval  CrossEntropyLoss |  4.14533615\n","Step   3350: eval          Accuracy |  0.25953630\n","\n","Step   3400: Ran 50 train steps in 36.17 secs\n","Step   3400: train CrossEntropyLoss |  4.12086868\n","Step   3400: eval  CrossEntropyLoss |  4.09660959\n","Step   3400: eval          Accuracy |  0.27412280\n","\n","Step   3450: Ran 50 train steps in 37.69 secs\n","Step   3450: train CrossEntropyLoss |  4.17257404\n","Step   3450: eval  CrossEntropyLoss |  4.36798906\n","Step   3450: eval          Accuracy |  0.23639609\n","\n","Step   3500: Ran 50 train steps in 37.12 secs\n","Step   3500: train CrossEntropyLoss |  4.17068768\n","Step   3500: eval  CrossEntropyLoss |  4.26203156\n","Step   3500: eval          Accuracy |  0.23044269\n","\n","Step   3550: Ran 50 train steps in 36.28 secs\n","Step   3550: train CrossEntropyLoss |  4.16747284\n","Step   3550: eval  CrossEntropyLoss |  3.98505259\n","Step   3550: eval          Accuracy |  0.26096180\n","\n","Step   3600: Ran 50 train steps in 35.95 secs\n","Step   3600: train CrossEntropyLoss |  4.18005991\n","Step   3600: eval  CrossEntropyLoss |  4.08206940\n","Step   3600: eval          Accuracy |  0.25367156\n","\n","Step   3650: Ran 50 train steps in 36.91 secs\n","Step   3650: train CrossEntropyLoss |  4.13448524\n","Step   3650: eval  CrossEntropyLoss |  4.01618004\n","Step   3650: eval          Accuracy |  0.26689190\n","\n","Step   3700: Ran 50 train steps in 36.08 secs\n","Step   3700: train CrossEntropyLoss |  4.12401915\n","Step   3700: eval  CrossEntropyLoss |  4.16557503\n","Step   3700: eval          Accuracy |  0.24796493\n","\n","Step   3750: Ran 50 train steps in 36.91 secs\n","Step   3750: train CrossEntropyLoss |  4.13014984\n","Step   3750: eval  CrossEntropyLoss |  4.10891628\n","Step   3750: eval          Accuracy |  0.25857520\n","\n","Step   3800: Ran 50 train steps in 36.50 secs\n","Step   3800: train CrossEntropyLoss |  4.08655357\n","Step   3800: eval  CrossEntropyLoss |  4.08665943\n","Step   3800: eval          Accuracy |  0.26735061\n","\n","Step   3850: Ran 50 train steps in 36.28 secs\n","Step   3850: train CrossEntropyLoss |  4.10891724\n","Step   3850: eval  CrossEntropyLoss |  3.95719337\n","Step   3850: eval          Accuracy |  0.25215349\n","\n","Step   3900: Ran 50 train steps in 36.86 secs\n","Step   3900: train CrossEntropyLoss |  4.04854107\n","Step   3900: eval  CrossEntropyLoss |  4.09722281\n","Step   3900: eval          Accuracy |  0.25986394\n","\n","Step   3950: Ran 50 train steps in 36.86 secs\n","Step   3950: train CrossEntropyLoss |  4.07398987\n","Step   3950: eval  CrossEntropyLoss |  3.95562029\n","Step   3950: eval          Accuracy |  0.26071101\n","\n","Step   4000: Ran 50 train steps in 36.35 secs\n","Step   4000: train CrossEntropyLoss |  4.05672216\n","Step   4000: eval  CrossEntropyLoss |  4.03542185\n","Step   4000: eval          Accuracy |  0.24965422\n","\n","Step   4050: Ran 50 train steps in 37.04 secs\n","Step   4050: train CrossEntropyLoss |  4.05155420\n","Step   4050: eval  CrossEntropyLoss |  3.96345901\n","Step   4050: eval          Accuracy |  0.26232615\n","\n","Step   4100: Ran 50 train steps in 36.87 secs\n","Step   4100: train CrossEntropyLoss |  4.02237177\n","Step   4100: eval  CrossEntropyLoss |  4.08639812\n","Step   4100: eval          Accuracy |  0.27964491\n","\n","Step   4150: Ran 50 train steps in 36.22 secs\n","Step   4150: train CrossEntropyLoss |  4.04692364\n","Step   4150: eval  CrossEntropyLoss |  4.05484390\n","Step   4150: eval          Accuracy |  0.24913256\n","\n","Step   4200: Ran 50 train steps in 35.59 secs\n","Step   4200: train CrossEntropyLoss |  4.03203297\n","Step   4200: eval  CrossEntropyLoss |  4.05414867\n","Step   4200: eval          Accuracy |  0.25584501\n","\n","Step   4250: Ran 50 train steps in 35.38 secs\n","Step   4250: train CrossEntropyLoss |  4.02254486\n","Step   4250: eval  CrossEntropyLoss |  3.97349286\n","Step   4250: eval          Accuracy |  0.25249171\n","\n","Step   4300: Ran 50 train steps in 35.36 secs\n","Step   4300: train CrossEntropyLoss |  4.01651525\n","Step   4300: eval  CrossEntropyLoss |  3.89113593\n","Step   4300: eval          Accuracy |  0.27543604\n","\n","Step   4350: Ran 50 train steps in 35.71 secs\n","Step   4350: train CrossEntropyLoss |  4.02691031\n","Step   4350: eval  CrossEntropyLoss |  3.94178057\n","Step   4350: eval          Accuracy |  0.28611112\n","\n","Step   4400: Ran 50 train steps in 36.31 secs\n","Step   4400: train CrossEntropyLoss |  3.99043941\n","Step   4400: eval  CrossEntropyLoss |  3.91944861\n","Step   4400: eval          Accuracy |  0.26482478\n","\n","Step   4450: Ran 50 train steps in 35.76 secs\n","Step   4450: train CrossEntropyLoss |  3.97862840\n","Step   4450: eval  CrossEntropyLoss |  4.00284243\n","Step   4450: eval          Accuracy |  0.27213821\n","\n","Step   4500: Ran 50 train steps in 35.60 secs\n","Step   4500: train CrossEntropyLoss |  3.99875116\n","Step   4500: eval  CrossEntropyLoss |  3.86129117\n","Step   4500: eval          Accuracy |  0.28071481\n","\n","Step   4550: Ran 50 train steps in 35.57 secs\n","Step   4550: train CrossEntropyLoss |  3.98531985\n","Step   4550: eval  CrossEntropyLoss |  4.05955696\n","Step   4550: eval          Accuracy |  0.24879725\n","\n","Step   4600: Ran 50 train steps in 35.56 secs\n","Step   4600: train CrossEntropyLoss |  3.98530817\n","Step   4600: eval  CrossEntropyLoss |  3.92796993\n","Step   4600: eval          Accuracy |  0.27114427\n","\n","Step   4650: Ran 50 train steps in 35.98 secs\n","Step   4650: train CrossEntropyLoss |  3.93953705\n","Step   4650: eval  CrossEntropyLoss |  3.96789479\n","Step   4650: eval          Accuracy |  0.28364116\n","\n","Step   4700: Ran 50 train steps in 35.48 secs\n","Step   4700: train CrossEntropyLoss |  3.94893551\n","Step   4700: eval  CrossEntropyLoss |  4.01560354\n","Step   4700: eval          Accuracy |  0.25654852\n","\n","Step   4750: Ran 50 train steps in 35.77 secs\n","Step   4750: train CrossEntropyLoss |  3.95669508\n","Step   4750: eval  CrossEntropyLoss |  3.91030598\n","Step   4750: eval          Accuracy |  0.27418360\n","\n","Step   4800: Ran 50 train steps in 35.34 secs\n","Step   4800: train CrossEntropyLoss |  3.98687434\n","Step   4800: eval  CrossEntropyLoss |  3.92078161\n","Step   4800: eval          Accuracy |  0.28293064\n","\n","Step   4850: Ran 50 train steps in 44.71 secs\n","Step   4850: train CrossEntropyLoss |  3.96274710\n","Step   4850: eval  CrossEntropyLoss |  3.97819257\n","Step   4850: eval          Accuracy |  0.27736625\n","\n","Step   4900: Ran 50 train steps in 36.91 secs\n","Step   4900: train CrossEntropyLoss |  3.94050503\n","Step   4900: eval  CrossEntropyLoss |  3.90047908\n","Step   4900: eval          Accuracy |  0.27927929\n","\n","Step   4950: Ran 50 train steps in 36.45 secs\n","Step   4950: train CrossEntropyLoss |  3.94033527\n","Step   4950: eval  CrossEntropyLoss |  3.96484685\n","Step   4950: eval          Accuracy |  0.29019347\n","\n","Step   5000: Ran 50 train steps in 35.14 secs\n","Step   5000: train CrossEntropyLoss |  3.96841574\n","Step   5000: eval  CrossEntropyLoss |  4.04869413\n","Step   5000: eval          Accuracy |  0.27251732\n","\n","Step   5050: Ran 50 train steps in 35.49 secs\n","Step   5050: train CrossEntropyLoss |  3.93864059\n","Step   5050: eval  CrossEntropyLoss |  3.95184660\n","Step   5050: eval          Accuracy |  0.26952791\n","\n","Step   5100: Ran 50 train steps in 36.15 secs\n","Step   5100: train CrossEntropyLoss |  3.97042251\n","Step   5100: eval  CrossEntropyLoss |  3.83947587\n","Step   5100: eval          Accuracy |  0.29200652\n","\n","Step   5150: Ran 50 train steps in 35.63 secs\n","Step   5150: train CrossEntropyLoss |  3.94877982\n","Step   5150: eval  CrossEntropyLoss |  3.91579628\n","Step   5150: eval          Accuracy |  0.28277439\n","\n","Step   5200: Ran 50 train steps in 35.76 secs\n","Step   5200: train CrossEntropyLoss |  3.94348478\n","Step   5200: eval  CrossEntropyLoss |  4.00255489\n","Step   5200: eval          Accuracy |  0.27815467\n","\n","Step   5250: Ran 50 train steps in 35.57 secs\n","Step   5250: train CrossEntropyLoss |  3.93277407\n","Step   5250: eval  CrossEntropyLoss |  3.86689258\n","Step   5250: eval          Accuracy |  0.28863797\n","\n","Step   5300: Ran 50 train steps in 34.99 secs\n","Step   5300: train CrossEntropyLoss |  3.92730713\n","Step   5300: eval  CrossEntropyLoss |  4.17709875\n","Step   5300: eval          Accuracy |  0.24586093\n","\n","Step   5350: Ran 50 train steps in 35.61 secs\n","Step   5350: train CrossEntropyLoss |  3.89778399\n","Step   5350: eval  CrossEntropyLoss |  3.93712354\n","Step   5350: eval          Accuracy |  0.28064516\n","\n","Step   5400: Ran 50 train steps in 36.05 secs\n","Step   5400: train CrossEntropyLoss |  3.90597057\n","Step   5400: eval  CrossEntropyLoss |  3.95940971\n","Step   5400: eval          Accuracy |  0.28439060\n","\n","Step   5450: Ran 50 train steps in 35.64 secs\n","Step   5450: train CrossEntropyLoss |  3.90275931\n","Step   5450: eval  CrossEntropyLoss |  3.85186553\n","Step   5450: eval          Accuracy |  0.27838260\n","\n","Step   5500: Ran 50 train steps in 35.54 secs\n","Step   5500: train CrossEntropyLoss |  3.88577056\n","Step   5500: eval  CrossEntropyLoss |  4.12628365\n","Step   5500: eval          Accuracy |  0.25815603\n","\n","Step   5550: Ran 50 train steps in 35.48 secs\n","Step   5550: train CrossEntropyLoss |  3.88268542\n","Step   5550: eval  CrossEntropyLoss |  3.86196613\n","Step   5550: eval          Accuracy |  0.29189560\n","\n","Step   5600: Ran 50 train steps in 35.51 secs\n","Step   5600: train CrossEntropyLoss |  3.85915160\n","Step   5600: eval  CrossEntropyLoss |  3.80911565\n","Step   5600: eval          Accuracy |  0.28978622\n","\n","Step   5650: Ran 50 train steps in 36.30 secs\n","Step   5650: train CrossEntropyLoss |  3.88667297\n","Step   5650: eval  CrossEntropyLoss |  3.81314588\n","Step   5650: eval          Accuracy |  0.30544490\n","\n","Step   5700: Ran 50 train steps in 35.44 secs\n","Step   5700: train CrossEntropyLoss |  3.88330770\n","Step   5700: eval  CrossEntropyLoss |  3.93697357\n","Step   5700: eval          Accuracy |  0.27867699\n","\n","Step   5750: Ran 50 train steps in 35.06 secs\n","Step   5750: train CrossEntropyLoss |  3.87420392\n","Step   5750: eval  CrossEntropyLoss |  3.80409884\n","Step   5750: eval          Accuracy |  0.28095537\n","\n","Step   5800: Ran 50 train steps in 35.75 secs\n","Step   5800: train CrossEntropyLoss |  3.82023120\n","Step   5800: eval  CrossEntropyLoss |  3.92147517\n","Step   5800: eval          Accuracy |  0.28814873\n","\n","Step   5850: Ran 50 train steps in 35.66 secs\n","Step   5850: train CrossEntropyLoss |  3.88112378\n","Step   5850: eval  CrossEntropyLoss |  3.90760660\n","Step   5850: eval          Accuracy |  0.27978009\n","\n","Step   5900: Ran 50 train steps in 35.49 secs\n","Step   5900: train CrossEntropyLoss |  3.87410951\n","Step   5900: eval  CrossEntropyLoss |  3.84147787\n","Step   5900: eval          Accuracy |  0.28244275\n","\n","Step   5950: Ran 50 train steps in 35.14 secs\n","Step   5950: train CrossEntropyLoss |  3.85757375\n","Step   5950: eval  CrossEntropyLoss |  3.82822061\n","Step   5950: eval          Accuracy |  0.29029796\n","\n","Step   6000: Ran 50 train steps in 35.97 secs\n","Step   6000: train CrossEntropyLoss |  3.84138298\n","Step   6000: eval  CrossEntropyLoss |  3.97235703\n","Step   6000: eval          Accuracy |  0.26655895\n","\n","Step   6050: Ran 50 train steps in 35.77 secs\n","Step   6050: train CrossEntropyLoss |  3.85011578\n","Step   6050: eval  CrossEntropyLoss |  3.76725817\n","Step   6050: eval          Accuracy |  0.28647014\n","\n","Step   6100: Ran 50 train steps in 35.84 secs\n","Step   6100: train CrossEntropyLoss |  3.83057427\n","Step   6100: eval  CrossEntropyLoss |  3.96473122\n","Step   6100: eval          Accuracy |  0.27336860\n","\n","Step   6150: Ran 50 train steps in 35.67 secs\n","Step   6150: train CrossEntropyLoss |  3.85201311\n","Step   6150: eval  CrossEntropyLoss |  3.72530031\n","Step   6150: eval          Accuracy |  0.30517709\n","\n","Step   6200: Ran 50 train steps in 36.31 secs\n","Step   6200: train CrossEntropyLoss |  3.82598591\n","Step   6200: eval  CrossEntropyLoss |  3.83783531\n","Step   6200: eval          Accuracy |  0.28497791\n","\n","Step   6250: Ran 50 train steps in 36.16 secs\n","Step   6250: train CrossEntropyLoss |  3.82906413\n","Step   6250: eval  CrossEntropyLoss |  3.80896854\n","Step   6250: eval          Accuracy |  0.31194690\n","\n","Step   6300: Ran 50 train steps in 35.40 secs\n","Step   6300: train CrossEntropyLoss |  3.81031203\n","Step   6300: eval  CrossEntropyLoss |  3.86682725\n","Step   6300: eval          Accuracy |  0.28016645\n","\n","Step   6350: Ran 50 train steps in 36.14 secs\n","Step   6350: train CrossEntropyLoss |  3.80553436\n","Step   6350: eval  CrossEntropyLoss |  3.67498994\n","Step   6350: eval          Accuracy |  0.29213482\n","\n","Step   6400: Ran 50 train steps in 35.22 secs\n","Step   6400: train CrossEntropyLoss |  3.80613852\n","Step   6400: eval  CrossEntropyLoss |  4.01143265\n","Step   6400: eval          Accuracy |  0.27950761\n","\n","Step   6450: Ran 50 train steps in 36.26 secs\n","Step   6450: train CrossEntropyLoss |  3.82379436\n","Step   6450: eval  CrossEntropyLoss |  3.92583823\n","Step   6450: eval          Accuracy |  0.26631260\n","\n","Step   6500: Ran 50 train steps in 35.27 secs\n","Step   6500: train CrossEntropyLoss |  3.79548097\n","Step   6500: eval  CrossEntropyLoss |  3.82050276\n","Step   6500: eval          Accuracy |  0.28166667\n","\n","Step   6550: Ran 50 train steps in 35.14 secs\n","Step   6550: train CrossEntropyLoss |  3.82574296\n","Step   6550: eval  CrossEntropyLoss |  3.60702610\n","Step   6550: eval          Accuracy |  0.30242512\n","\n","Step   6600: Ran 50 train steps in 35.42 secs\n","Step   6600: train CrossEntropyLoss |  3.81880212\n","Step   6600: eval  CrossEntropyLoss |  3.74851966\n","Step   6600: eval          Accuracy |  0.28985506\n","\n","Step   6650: Ran 50 train steps in 36.36 secs\n","Step   6650: train CrossEntropyLoss |  3.79696774\n","Step   6650: eval  CrossEntropyLoss |  3.68647790\n","Step   6650: eval          Accuracy |  0.30346632\n","\n","Step   6700: Ran 50 train steps in 35.44 secs\n","Step   6700: train CrossEntropyLoss |  3.81246209\n","Step   6700: eval  CrossEntropyLoss |  3.87374520\n","Step   6700: eval          Accuracy |  0.28442147\n","\n","Step   6750: Ran 50 train steps in 36.44 secs\n","Step   6750: train CrossEntropyLoss |  3.77940845\n","Step   6750: eval  CrossEntropyLoss |  3.73907042\n","Step   6750: eval          Accuracy |  0.29802170\n","\n","Step   6800: Ran 50 train steps in 35.86 secs\n","Step   6800: train CrossEntropyLoss |  3.78710556\n","Step   6800: eval  CrossEntropyLoss |  4.04070520\n","Step   6800: eval          Accuracy |  0.28040540\n","\n","Step   6850: Ran 50 train steps in 37.62 secs\n","Step   6850: train CrossEntropyLoss |  3.78395104\n","Step   6850: eval  CrossEntropyLoss |  3.87707353\n","Step   6850: eval          Accuracy |  0.30601886\n","\n","Step   6900: Ran 50 train steps in 36.35 secs\n","Step   6900: train CrossEntropyLoss |  3.78613973\n","Step   6900: eval  CrossEntropyLoss |  3.71144104\n","Step   6900: eval          Accuracy |  0.28928372\n","\n","Step   6950: Ran 50 train steps in 36.00 secs\n","Step   6950: train CrossEntropyLoss |  3.78534102\n","Step   6950: eval  CrossEntropyLoss |  3.65747643\n","Step   6950: eval          Accuracy |  0.32159266\n","\n","Step   7000: Ran 50 train steps in 36.99 secs\n","Step   7000: train CrossEntropyLoss |  3.77616239\n","Step   7000: eval  CrossEntropyLoss |  3.63933897\n","Step   7000: eval          Accuracy |  0.31616595\n","\n","Step   7050: Ran 50 train steps in 36.20 secs\n","Step   7050: train CrossEntropyLoss |  3.74264216\n","Step   7050: eval  CrossEntropyLoss |  3.67888999\n","Step   7050: eval          Accuracy |  0.27922079\n","\n","Step   7100: Ran 50 train steps in 35.76 secs\n","Step   7100: train CrossEntropyLoss |  3.77060986\n","Step   7100: eval  CrossEntropyLoss |  3.78373933\n","Step   7100: eval          Accuracy |  0.28322148\n","\n","Step   7150: Ran 50 train steps in 36.04 secs\n","Step   7150: train CrossEntropyLoss |  3.77900624\n","Step   7150: eval  CrossEntropyLoss |  3.77319002\n","Step   7150: eval          Accuracy |  0.31426448\n","\n","Step   7200: Ran 50 train steps in 35.53 secs\n","Step   7200: train CrossEntropyLoss |  3.77908540\n","Step   7200: eval  CrossEntropyLoss |  3.92569852\n","Step   7200: eval          Accuracy |  0.27597404\n","\n","Step   7250: Ran 50 train steps in 35.60 secs\n","Step   7250: train CrossEntropyLoss |  3.80287242\n","Step   7250: eval  CrossEntropyLoss |  3.72862267\n","Step   7250: eval          Accuracy |  0.28299120\n","\n","Step   7300: Ran 50 train steps in 36.26 secs\n","Step   7300: train CrossEntropyLoss |  3.75732207\n","Step   7300: eval  CrossEntropyLoss |  3.81015396\n","Step   7300: eval          Accuracy |  0.28591955\n","\n","Step   7350: Ran 50 train steps in 35.82 secs\n","Step   7350: train CrossEntropyLoss |  3.75884175\n","Step   7350: eval  CrossEntropyLoss |  3.63245058\n","Step   7350: eval          Accuracy |  0.29875824\n","\n","Step   7400: Ran 50 train steps in 36.52 secs\n","Step   7400: train CrossEntropyLoss |  3.77648664\n","Step   7400: eval  CrossEntropyLoss |  3.75520778\n","Step   7400: eval          Accuracy |  0.29796422\n","\n","Step   7450: Ran 50 train steps in 35.72 secs\n","Step   7450: train CrossEntropyLoss |  3.74645114\n","Step   7450: eval  CrossEntropyLoss |  3.67260027\n","Step   7450: eval          Accuracy |  0.32241076\n","\n","Step   7500: Ran 50 train steps in 35.49 secs\n","Step   7500: train CrossEntropyLoss |  3.74688220\n","Step   7500: eval  CrossEntropyLoss |  3.68687177\n","Step   7500: eval          Accuracy |  0.31582490\n","\n","Step   7550: Ran 50 train steps in 35.66 secs\n","Step   7550: train CrossEntropyLoss |  3.76463056\n","Step   7550: eval  CrossEntropyLoss |  3.72575736\n","Step   7550: eval          Accuracy |  0.30491328\n","\n","Step   7600: Ran 50 train steps in 35.72 secs\n","Step   7600: train CrossEntropyLoss |  3.76299071\n","Step   7600: eval  CrossEntropyLoss |  3.74364495\n","Step   7600: eval          Accuracy |  0.30015314\n","\n","Step   7650: Ran 50 train steps in 35.33 secs\n","Step   7650: train CrossEntropyLoss |  3.72818375\n","Step   7650: eval  CrossEntropyLoss |  3.85417747\n","Step   7650: eval          Accuracy |  0.28424659\n","\n","Step   7700: Ran 50 train steps in 36.14 secs\n","Step   7700: train CrossEntropyLoss |  3.73105860\n","Step   7700: eval  CrossEntropyLoss |  3.73626709\n","Step   7700: eval          Accuracy |  0.30962342\n","\n","Step   7750: Ran 50 train steps in 36.84 secs\n","Step   7750: train CrossEntropyLoss |  3.74184585\n","Step   7750: eval  CrossEntropyLoss |  3.60098052\n","Step   7750: eval          Accuracy |  0.31073445\n","\n","Step   7800: Ran 50 train steps in 35.42 secs\n","Step   7800: train CrossEntropyLoss |  3.74136567\n","Step   7800: eval  CrossEntropyLoss |  3.77244091\n","Step   7800: eval          Accuracy |  0.28551137\n","\n","Step   7850: Ran 50 train steps in 35.45 secs\n","Step   7850: train CrossEntropyLoss |  3.73252654\n","Step   7850: eval  CrossEntropyLoss |  3.61128020\n","Step   7850: eval          Accuracy |  0.29934925\n","\n","Step   7900: Ran 50 train steps in 35.36 secs\n","Step   7900: train CrossEntropyLoss |  3.71923709\n","Step   7900: eval  CrossEntropyLoss |  3.74443936\n","Step   7900: eval          Accuracy |  0.27570924\n","\n","Step   7950: Ran 50 train steps in 36.26 secs\n","Step   7950: train CrossEntropyLoss |  3.71221948\n","Step   7950: eval  CrossEntropyLoss |  3.62281322\n","Step   7950: eval          Accuracy |  0.30758622\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kIj-T06Utx4o","executionInfo":{"status":"aborted","timestamp":1608259231112,"user_tz":300,"elapsed":10887,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":[""],"execution_count":null,"outputs":[]}]}