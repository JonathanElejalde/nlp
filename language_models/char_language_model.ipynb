{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"char_language_model.ipynb","provenance":[],"mount_file_id":"18j2CYzlVkh5xvVv4AVgeCnkXCdtQLXT_","authorship_tag":"ABX9TyP5M093LInvfOdTrnqzayjP"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ltEvDmQujeg0","executionInfo":{"status":"ok","timestamp":1607618989651,"user_tz":300,"elapsed":6623,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"0cce629e-bc7c-4df2-d562-8ff7254d2da2"},"source":["%cd /content/drive/My Drive/Colab Notebooks/nlp/apps/language_models"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/nlp/apps/language_models\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P9HgnNO5jtKl"},"source":["# What we need to do\r\n","\r\n","- You will start by converting a line of text into a tensor\r\n","- Then you will create a generator to feed data into the model\r\n","- You will train a neural network in order to predict the new set of characters of defined length.\r\n","- You will use embeddings for each character and feed them as inputs to your model.\r\n","    - Many natural language tasks rely on using embeddings for predictions.\r\n","\r\n","- Your model will convert each character to its embedding, run the embeddings through a Gated Recurrent Unit GRU, and run it through a linear layer to predict the next set of characters.\r\n","\r\n","- You will get the embeddings;\r\n","- Stack the embeddings on top of each other;\r\n","- Run them through two layers with a relu activation in the middle;\r\n","- Finally, you will compute the softmax. \r\n","\r\n","To predict the next character:\r\n","- Use the softmax output and identify the word with the highest probability.\r\n","- The word with the highest probability is the prediction for the next word."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PkzpbTWrkQM4","executionInfo":{"status":"ok","timestamp":1607619172067,"user_tz":300,"elapsed":13329,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"87534e3d-0e94-4159-f5ab-a0ec1127868a"},"source":["!pip install -q -U trax"],"execution_count":3,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 471kB 9.0MB/s \n","\u001b[K     |████████████████████████████████| 2.6MB 15.7MB/s \n","\u001b[K     |████████████████████████████████| 174kB 47.1MB/s \n","\u001b[K     |████████████████████████████████| 3.7MB 40.6MB/s \n","\u001b[K     |████████████████████████████████| 71kB 8.5MB/s \n","\u001b[K     |████████████████████████████████| 1.1MB 26.5MB/s \n","\u001b[K     |████████████████████████████████| 348kB 43.0MB/s \n","\u001b[K     |████████████████████████████████| 1.4MB 38.4MB/s \n","\u001b[K     |████████████████████████████████| 2.9MB 46.3MB/s \n","\u001b[K     |████████████████████████████████| 890kB 39.0MB/s \n","\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XD0hpHk5j4Ia","executionInfo":{"status":"ok","timestamp":1607625998470,"user_tz":300,"elapsed":806,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["import trax\r\n","import trax.fastmath.numpy as np\r\n","import numpy\r\n","import random\r\n","from trax import fastmath\r\n","from trax import layers as tl"],"execution_count":50,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4e6K4mnBkvKo"},"source":["# Get the data\r\n","\r\n","We will treat the sherlock novels are our data. Then, we will treat each line as a sentence, because we are going to predict characters instead of words, we need to convert each sentence into characters. After this, each character line is going to be stored in a list. In other words, we are going to have a list of list of characters.       \r\n","Finally, we will create a generator that takes the batch_size and max_length. Where max_length is the sentence with the maximum size.\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"cAn-L03Ykyv_","executionInfo":{"status":"ok","timestamp":1607619638015,"user_tz":300,"elapsed":924,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/sherlock_novels.txt'\r\n","testing_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/study in scarlet.txt'"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"ABPB3lsRmO8l","executionInfo":{"status":"ok","timestamp":1607619928294,"user_tz":300,"elapsed":803,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def get_sentences(path):\r\n","    \"\"\"\r\n","    Reads a txt file and returns each line (sentence)\r\n","    in a list\r\n","    \"\"\"\r\n","    with open(path) as f:\r\n","        sentences = f.readlines()\r\n","\r\n","    return sentences\r\n"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FIn0h4-6nMb5"},"source":["# Preprocess\r\n"]},{"cell_type":"code","metadata":{"id":"J7OrKdZuoGW9","executionInfo":{"status":"ok","timestamp":1607620357351,"user_tz":300,"elapsed":531,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def preprocess(sentences):\r\n","    \"\"\"\r\n","    Takes a list of sentences to clean and lowercase them\r\n","    \"\"\"\r\n","    for i, sentence in enumerate(sentences):\r\n","        sentences[i] = sentence.strip().lower()\r\n","\r\n","    return sentences\r\n"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g60CniXloGdx"},"source":["# Create validation and test set"]},{"cell_type":"code","metadata":{"id":"2DaNnMK4oGgz","executionInfo":{"status":"ok","timestamp":1607625810662,"user_tz":300,"elapsed":838,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def create_train_val(sentences):\r\n","    \"\"\"\r\n","    It takes a list of sentences and divides them into\r\n","    90% train and 10% validation\r\n","    \"\"\"\r\n","    n = len(sentences)\r\n","    pct = int(n * 0.9)\r\n","    train = sentences[:pct]\r\n","    validation = sentences[pct:]\r\n","\r\n","    return train, validation"],"execution_count":49,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EkLAoaGDqDuA"},"source":["# Convert sentences to tensors\r\n","\r\n","Now, we need to convert our sentences into numbers, thus we can feed them into our model."]},{"cell_type":"code","metadata":{"id":"baVdUjZiqDz0","executionInfo":{"status":"ok","timestamp":1607622658698,"user_tz":300,"elapsed":621,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def sentence2tensor(sentence, end_token=1):\r\n","    \"\"\"\r\n","    It takes the sentence and transforms each\r\n","    character to a number\r\n","    \"\"\"\r\n","    tensor = [ord(char) for char in sentence]\r\n","    # append the end token to the sentence\r\n","    tensor.append(end_token)\r\n","\r\n","    return tensor\r\n","\r\n","    \r\n","    \r\n"],"execution_count":43,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7UNU5sr0xxUQ"},"source":["# Generate batches\r\n","\r\n","We will convert our text sentences into numpy arrays and we will add padding to each sentence. This padding will be determine by the sentence with the max_length in our corpus.\r\n","\r\n","The batch is a tuple with three values: inputs, targets, mask. Mask will be 1 for all non-padding tokens."]},{"cell_type":"code","metadata":{"id":"93Md-_nLxxXa"},"source":["def generate_batch(batch_size, max_length, sentences, sentence2tensor=sentence2tensor, shuffle=True):\r\n","    \"\"\"\r\n","    It takes a list of sentences, \r\n","    \"\"\"\r\n","    index = 0\r\n","    current_batch = []\r\n","    num_sentences = len(sentences)\r\n","\r\n","    # create an array with the indexes of sentences that can be shuffled\r\n","    sentences_index = [*range(num_lines)]\r\n","\r\n","    if shuffle:\r\n","        random.shuffle(sentences_index)\r\n","\r\n","    while True:\r\n","        if index >= num_sentences:\r\n","            # reset index if we used all the sentences\r\n","            index = 0\r\n","\r\n","            if shuffle:\r\n","                random.shuffle(sentences_index)\r\n","\r\n","        # get a sentence\r\n","        sentence = sentences[sentences_index[index]]\r\n","\r\n","        if len(sentence) < max_length:\r\n","            current_batch.append(sentence)\r\n","\r\n","        index += 1\r\n","\r\n","        # check if we already have our desire batch_size\r\n","        if len(current_batch) == batch_size:\r\n","            batch = []\r\n","            mask = []\r\n","            for batch_sentence in current_batch:\r\n","                # convert the batch sentence to a tensor\r\n","                tensor = sentence2tensor(batch_sentence)\r\n","\r\n","                # add the padding\r\n","                pad = [0] * (max_lenght - len(tensor))\r\n","                tensor_padded = tensor + pad\r\n","\r\n","                batch.append(tensor_padded)\r\n","                mask_tensor = [0 if i == 0 else 1 for i in tensor_padded]\r\n","                mask.append(mask_tensor)\r\n","\r\n","            # convert the padded tensor into a trax tensor\r\n","            trax_batch = np.array(batch)\r\n","            trax_mask = np.array(mask)\r\n","\r\n","            # yield two copies of the batch and mask\r\n","            yield trax_batch, trax_batch, trax_mask\r\n","\r\n","            # reset current_batch to an empty list\r\n","            current_batch = []\r\n","            \r\n","\r\n","    \r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"alS1m5rZxxZ7"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7bgZCbAxnGM7","executionInfo":{"status":"ok","timestamp":1607622716860,"user_tz":300,"elapsed":662,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"93280833-b2cb-4181-9f18-6caa76a202c8"},"source":[" sentences = get_sentences(testing_path)\r\n","# n_sen = len(sen)\r\n","# print(f\"Number of sen: {n_sen}\")\r\n","# print(f\"Sample line at position 0 {sen[0]}\")\r\n","# print(f\"Sample line at position 1000 {sen[1000]}\")\r\n","\r\n","sentences = preprocess(sentences)\r\n","train, val = create_train_val(sentences)\r\n","# print(len(train))\r\n","# print(len(val))\r\n","\r\n","tensor = sentence2tensor(train[1000])\r\n","print(tensor)\r\n"],"execution_count":45,"outputs":[{"output_type":"stream","text":["[34, 116, 104, 101, 32, 102, 105, 110, 103, 101, 114, 32, 110, 97, 105, 108, 115, 32, 97, 110, 100, 32, 116, 104, 101, 32, 116, 114, 105, 99, 104, 105, 110, 111, 112, 111, 108, 121, 44, 34, 32, 105, 32, 115, 117, 103, 103, 101, 115, 116, 101, 100, 46, 1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vsWETtBNyDnB"},"source":[""],"execution_count":null,"outputs":[]}]}