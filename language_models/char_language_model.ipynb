{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"char_language_model.ipynb","provenance":[],"mount_file_id":"18j2CYzlVkh5xvVv4AVgeCnkXCdtQLXT_","authorship_tag":"ABX9TyO+d4hfkAd3Sr+kSQYsNoUP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ltEvDmQujeg0","executionInfo":{"status":"ok","timestamp":1607643363228,"user_tz":300,"elapsed":580,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"5fe27ef3-1f7f-40a9-b4df-508fb2c5659c"},"source":["%cd /content/drive/My Drive/Colab Notebooks/nlp/apps/language_models"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/nlp/apps/language_models\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P9HgnNO5jtKl"},"source":["# What we need to do\r\n","\r\n","- You will start by converting a line of text into a tensor\r\n","- Then you will create a generator to feed data into the model\r\n","- You will train a neural network in order to predict the new set of characters of defined length.\r\n","- You will use embeddings for each character and feed them as inputs to your model.\r\n","    - Many natural language tasks rely on using embeddings for predictions.\r\n","\r\n","- Your model will convert each character to its embedding, run the embeddings through a Gated Recurrent Unit GRU, and run it through a linear layer to predict the next set of characters.\r\n","\r\n","- You will get the embeddings;\r\n","- Stack the embeddings on top of each other;\r\n","- Run them through two layers with a relu activation in the middle;\r\n","- Finally, you will compute the softmax. \r\n","\r\n","To predict the next character:\r\n","- Use the softmax output and identify the word with the highest probability.\r\n","- The word with the highest probability is the prediction for the next word."]},{"cell_type":"code","metadata":{"id":"PkzpbTWrkQM4","executionInfo":{"status":"ok","timestamp":1607643367555,"user_tz":300,"elapsed":3602,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["!pip install -q -U trax"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XD0hpHk5j4Ia","executionInfo":{"status":"ok","timestamp":1607643378867,"user_tz":300,"elapsed":14622,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"978ee1f9-ac7c-485e-e830-29eed6e1b275"},"source":["import trax\r\n","import trax.fastmath.numpy as np\r\n","import numpy\r\n","import random\r\n","import itertools\r\n","from trax import fastmath"],"execution_count":3,"outputs":[{"output_type":"stream","text":["WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"4e6K4mnBkvKo"},"source":["# Get the data\r\n","\r\n","We will treat the sherlock novels are our data. Then, we will treat each line as a sentence, because we are going to predict characters instead of words, we need to convert each sentence into characters. After this, each character line is going to be stored in a list. In other words, we are going to have a list of list of characters.       \r\n","Finally, we will create a generator that takes the batch_size and max_length. Where max_length is the sentence with the maximum size.\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"cAn-L03Ykyv_","executionInfo":{"status":"ok","timestamp":1607643378869,"user_tz":300,"elapsed":13960,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/sherlock_novels.txt'\r\n","testing_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/study in scarlet.txt'\r\n","output_dir = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/language_models/models'"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"ABPB3lsRmO8l","executionInfo":{"status":"ok","timestamp":1607643378887,"user_tz":300,"elapsed":13349,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def get_sentences(path):\r\n","    \"\"\"\r\n","    Reads a txt file and returns each line (sentence)\r\n","    in a list\r\n","    \"\"\"\r\n","    with open(path) as f:\r\n","        sentences = f.readlines()\r\n","\r\n","    return sentences\r\n","\r\n","def get_max_length(sentences):\r\n","    \"\"\"\r\n","    Takes a list of sentences and search for the\r\n","    longest one.\r\n","    \"\"\"\r\n","    sentence = max(sentences, key=len)\r\n","    max_length = len(sentence)\r\n","    return max_length, sentence\r\n"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FIn0h4-6nMb5"},"source":["# Preprocess\r\n","\r\n"]},{"cell_type":"code","metadata":{"id":"J7OrKdZuoGW9","executionInfo":{"status":"ok","timestamp":1607643378888,"user_tz":300,"elapsed":12686,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def preprocess(sentences):\r\n","    \"\"\"\r\n","    Takes a list of sentences to clean and lowercase them\r\n","    \"\"\"\r\n","    for i, sentence in enumerate(sentences):\r\n","        sentences[i] = sentence.strip().lower()\r\n","\r\n","    return sentences\r\n"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g60CniXloGdx"},"source":["# Create validation and test set"]},{"cell_type":"code","metadata":{"id":"2DaNnMK4oGgz","executionInfo":{"status":"ok","timestamp":1607643378889,"user_tz":300,"elapsed":12004,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def create_train_val(sentences):\r\n","    \"\"\"\r\n","    It takes a list of sentences and divides them into\r\n","    90% train and 10% validation\r\n","    \"\"\"\r\n","    n = len(sentences)\r\n","    pct = int(n * 0.9)\r\n","    train = sentences[:pct]\r\n","    validation = sentences[pct:]\r\n","\r\n","    return train, validation"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EkLAoaGDqDuA"},"source":["# Convert sentences to tensors\r\n","\r\n","Now, we need to convert our sentences into numbers, thus we can feed them into our model."]},{"cell_type":"code","metadata":{"id":"baVdUjZiqDz0","executionInfo":{"status":"ok","timestamp":1607643378890,"user_tz":300,"elapsed":11497,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def sentence2tensor(sentence, end_token=1):\r\n","    \"\"\"\r\n","    It takes the sentence and transforms each\r\n","    character to a number\r\n","    \"\"\"\r\n","    tensor = [ord(char) for char in sentence]\r\n","    # append the end token to the sentence\r\n","    tensor.append(end_token)\r\n","\r\n","    return tensor\r\n","\r\n","    \r\n","    \r\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7UNU5sr0xxUQ"},"source":["# Generate batches\r\n","\r\n","We will convert our text sentences into numpy arrays and we will add padding to each sentence. This padding will be determine by the sentence with the max_length in our corpus.\r\n","\r\n","The batch is a tuple with three values: inputs, targets, mask. Mask will be 1 for all non-padding tokens."]},{"cell_type":"code","metadata":{"id":"93Md-_nLxxXa","executionInfo":{"status":"ok","timestamp":1607643378891,"user_tz":300,"elapsed":10609,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def generate_batch(batch_size, max_length, sentences, sentence2tensor=sentence2tensor, shuffle=True):\r\n","    \"\"\"\r\n","    It takes a list of sentences, \r\n","    \"\"\"\r\n","    index = 0\r\n","    current_batch = []\r\n","    num_sentences = len(sentences)\r\n","\r\n","    # create an array with the indexes of sentences that can be shuffled\r\n","    sentences_index = [*range(num_sentences)]\r\n","\r\n","    if shuffle:\r\n","        random.shuffle(sentences_index)\r\n","\r\n","    while True:\r\n","        if index >= num_sentences:\r\n","            # reset index if we used all the sentences\r\n","            index = 0\r\n","\r\n","            if shuffle:\r\n","                random.shuffle(sentences_index)\r\n","\r\n","        # get a sentence\r\n","        sentence = sentences[sentences_index[index]]\r\n","\r\n","        if len(sentence) < max_length:\r\n","            current_batch.append(sentence)\r\n","\r\n","        index += 1\r\n","\r\n","        # check if we already have our desire batch_size\r\n","        if len(current_batch) == batch_size:\r\n","            batch = []\r\n","            mask = []\r\n","            for batch_sentence in current_batch:\r\n","                # convert the batch sentence to a tensor\r\n","                tensor = sentence2tensor(batch_sentence)\r\n","\r\n","                # add the padding\r\n","                pad = [0] * (max_length - len(tensor))\r\n","                tensor_padded = tensor + pad\r\n","\r\n","                batch.append(tensor_padded)\r\n","                mask_tensor = [0 if i == 0 else 1 for i in tensor_padded]\r\n","                mask.append(mask_tensor)\r\n","\r\n","            # convert the padded tensor into a trax tensor\r\n","            trax_batch = np.array(batch)\r\n","            trax_mask = np.array(mask)\r\n","\r\n","            # yield two copies of the batch and mask\r\n","            yield trax_batch, trax_batch, trax_mask\r\n","\r\n","            # reset current_batch to an empty list\r\n","            current_batch = []\r\n","                \r\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lZRtzOI_NIE8"},"source":["Once we have our function to generate batches, we need to a way to cycle over them and thus we create multiple epochs. One way to do it is with the itertools.cycle() function.\r\n","\r\n","```python\r\n","import itertools\r\n","\r\n","infinite_generator = itertools.cycle(generate_batch(batch_size, max_length, sentences))\r\n","```"]},{"cell_type":"markdown","metadata":{"id":"aD6KHbvvNIJF"},"source":["# Create the model"]},{"cell_type":"code","metadata":{"id":"vNrzOA8zNIMc","executionInfo":{"status":"ok","timestamp":1607643378891,"user_tz":300,"elapsed":9557,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def create_model(vocab_size=256, emb_dim=300, n_layers=2, mode='train'):\r\n","    \"\"\"\r\n","    Returns a GRU model.\r\n","    Args:\r\n","        vocab_size: int. the amount of unique char\r\n","        emb_dim: int. embeddings depth\r\n","        n_layers: int. number of GRU layers\r\n","        mode: str\r\n","    returns:\r\n","        model: a trax model\r\n","    \"\"\"\r\n","    model = trax.layers.Serial(\r\n","        trax.layers.ShiftRight(mode=mode),\r\n","        trax.layers.Embedding(vocab_size, emb_dim),\r\n","        [trax.layers.GRU(emb_dim) for _ in range(n_layers)],\r\n","        trax.layers.Dense(vocab_size),\r\n","        trax.layers.LogSoftmax()\r\n","    )\r\n","\r\n","    return model\r\n","\r\n"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DH1vABs_UT94"},"source":["# Training"]},{"cell_type":"code","metadata":{"id":"Cw3p6FbbUUBq","executionInfo":{"status":"ok","timestamp":1607643378906,"user_tz":300,"elapsed":8845,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["sentences = get_sentences(path)\r\n","pre_sentences = preprocess(sentences)\r\n","# get_max_length returns the len and the sentence\r\n","max_length, _ = get_max_length(pre_sentences)\r\n","batch_size = 32\r\n","train, val = create_train_val(pre_sentences)\r\n","\r\n","num_sentences = len(train)\r\n","n_steps = int(num_sentences / batch_size)\r\n","epochs = 100\r\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"qPF72hubUUEz","executionInfo":{"status":"ok","timestamp":1607643378908,"user_tz":300,"elapsed":8019,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["# num_sentences = len(pre_sentences)\r\n","# print(num_sentences)\r\n","# print(int(num_sentences / batch_size))\r\n","\r\n","def train_model(model, train_sentences, val_sentences, generate_batch, \r\n","                max_lenght, learning_rate= 0.0001, batch_size=32, n_steps=1, output_dir='models/'):\r\n","    \"\"\"\r\n","    It trains our trax model\r\n","    Args:\r\n","        model: trax model\r\n","        train_sentences: list\r\n","        val_sentences: list\r\n","        generate_batch: func\r\n","        max_length: int. it is the max length of the longest sentence\r\n","        batch_size: int\r\n","        n_steps: int. Number of steps to perform\r\n","        output_dir: str\r\n","    returns:\r\n","        a trax Training loop for the model.\r\n","    \"\"\"\r\n","    print(f'This is the amount of steps needed to end traning: {n_steps}')\r\n","    train_generator = generate_batch(batch_size, max_length, sentences)\r\n","    infinite_train_generator = itertools.cycle(train_generator)\r\n","\r\n","    val_generator = generate_batch(batch_size, max_length, val_sentences)\r\n","    infinite_val_generator = itertools.cycle(val_generator)\r\n","\r\n","    train_task = trax.supervised.training.TrainTask(\r\n","        labeled_data=infinite_train_generator,\r\n","        loss_layer=trax.layers.CrossEntropyLoss(),\r\n","        optimizer=trax.optimizers.Adam(learning_rate),\r\n","        n_steps_per_checkpoint=500\r\n","    )\r\n","\r\n","    val_task = trax.supervised.training.EvalTask(\r\n","        labeled_data=infinite_val_generator,\r\n","        metrics=[trax.layers.CrossEntropyLoss(), trax.layers.Accuracy()],\r\n","        n_eval_batches=10\r\n","    )\r\n","\r\n","    training_loop = trax.supervised.training.Loop(model, train_task, \r\n","                                                  eval_tasks=val_task, output_dir=output_dir)\r\n","    \r\n","    training_loop.run(n_steps=n_steps)\r\n","\r\n","    return training_loop\r\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"183VhYORUUHk","executionInfo":{"status":"ok","timestamp":1607643378909,"user_tz":300,"elapsed":7333,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["# training_loop = train_model(create_model(), train, val, generate_batch, max_length, learning_rate=0.001, \r\n","#                             n_steps=n_steps * epochs, output_dir=output_dir)"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oV3HzhAWhJj3"},"source":["# Evaluation\r\n","\r\n","To evaluate language models, we usually use perplexity which is a measure of how well a probability model predicts a sample. \r\n","\r\n","$$P(W) = \\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}$$\r\n","\r\n","As an implementation hack, we would usually take the log of that formula (to enable us to use the log probabilities we get as output of our `RNN`, convert exponents to products, and products into sums which makes computations less complicated and computationally more efficient). We should also take care of the padding, since we do not want to include the padding when calculating the perplexity (because we do not want to have a perplexity measure artificially good).\r\n","\r\n","\r\n","$$log P(W) = {log\\big(\\sqrt[N]{\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\big)}$$\r\n","\r\n","$$ = {log\\big({\\prod_{i=1}^{N} \\frac{1}{P(w_i| w_1,...,w_{n-1})}}\\big)^{\\frac{1}{N}}}$$ \r\n","\r\n","$$ = {log\\big({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\big)^{-\\frac{1}{N}}} $$\r\n","$$ = -\\frac{1}{N}{log\\big({\\prod_{i=1}^{N}{P(w_i| w_1,...,w_{n-1})}}\\big)} $$\r\n","$$ = -\\frac{1}{N}{\\big({\\sum_{i=1}^{N}{logP(w_i| w_1,...,w_{n-1})}}\\big)} $$"]},{"cell_type":"code","metadata":{"id":"b4EwwiCIhJp-","executionInfo":{"status":"ok","timestamp":1607643726311,"user_tz":300,"elapsed":519,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def calculate_perplexity(predictions, targets):\r\n","    \"\"\"\r\n","    It takes the predictions and targets. Where predictions\r\n","    is a tensor of log probabilities.\r\n","    Args:\r\n","        predictions: trax.array. These are the predictions of a list\r\n","            of batches of tensors corresponding to the sentences of the tex\r\n","        targets: trax.array. These are the actual list of batches corresponding\r\n","            to the sentences of the text\r\n","    returns:\r\n","        log_perplexity: float. This is the log perplexity of our model\r\n","    \"\"\"\r\n","    # we use trax.layers.one_hot to transform the target into the same dimension\r\n","    total_log_perplexity = np.sum(predictions * trax.layers.one_hot(targets, predictions.shape[-1]), axis=-1)\r\n","    non_pad = 1.0 - np.equal(targets, 0)\r\n","\r\n","    # get rid of the padding\r\n","    perplexity = total_log_perplexity * non_pad\r\n","    log_perplexity = np.sum(perplexity) / np.sum(non_pad)\r\n","\r\n","    return -log_perplexity\r\n"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hW12z99Y-mxW","executionInfo":{"status":"ok","timestamp":1607643729038,"user_tz":300,"elapsed":1941,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"ceb86b84-cbb9-44cc-c412-879a6b5f7206"},"source":["# Testing \r\n","model = create_model()\r\n","model.init_from_file(output_dir + '/' + 'model.pkl.gz')\r\n","batch = next(generate_batch(batch_size, max_length, pre_sentences, shuffle=False))\r\n","preds = model(batch[0])\r\n","log_ppx = calculate_perplexity(preds, batch[1])\r\n","print('The log perplexity and perplexity of your model are respectively', log_ppx, np.exp(log_ppx))"],"execution_count":26,"outputs":[{"output_type":"stream","text":["The log perplexity and perplexity of your model are respectively 1.0361327 2.8182967\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8HVt4sCUzJH0"},"source":["# Generate text with the model\r\n","\r\n","We will now use our own language model to generate new sentences for that we need to make draws from a Gumble distribution.\r\n","\r\n","The Gumbel Probability Density Function (PDF) is defined as: \r\n","\r\n","$$ f(z) = {1\\over{\\beta}}e^{(-z+e^{(-z)})} $$\r\n","\r\n","where: $$ z = {(x - \\mu)\\over{\\beta}}$$\r\n","\r\n","The maximum value, which is what we choose as the prediction in the last step of a Recursive Neural Network `RNN` we are using for text generation, in a sample of a random variable following an exponential distribution approaches the Gumbel distribution when the sample increases asymptotically. For that reason, the Gumbel distribution is used to sample from a categorical distribution."]},{"cell_type":"code","metadata":{"id":"VwF6Lgf4zJdN","executionInfo":{"status":"ok","timestamp":1607647423082,"user_tz":300,"elapsed":668,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def gumbel_sample(log_probabilities, temperature=1.0):\r\n","    \"\"\"Gumbel sampling from a categorical distribution.\"\"\"\r\n","    u = numpy.random.uniform(low=1e-6, high=1.0 - 1e-6, size=log_probabilities.shape)\r\n","    g = -np.log(-np.log(u))\r\n","\r\n","    return np.argmax(log_probabilities + g * temperature, axis=-1)\r\n","\r\n","def prediction(num_chars, prefix):\r\n","    inp = [ord(char) for char in prefix]\r\n","    result = [c for c in prefix]\r\n","    max_len = len(prefix) + num_chars\r\n","    for _ in range(num_chars):\r\n","        cur_inp = np.array(inp + [0] * (max_len - len(inp)))\r\n","        outp = model(cur_inp[None, :])  # Add batch dim.\r\n","        next_char = gumbel_sample(outp[0, len(inp)])\r\n","        inp += [int(next_char)]\r\n","       \r\n","        if inp[-1] == 1:\r\n","            break # EOS\r\n","        result.append(chr(int(next_char)))\r\n","    \r\n","    return \"\".join(result)\r\n","\r\n"],"execution_count":44,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FYlW4zTwzJha","executionInfo":{"status":"ok","timestamp":1607647627145,"user_tz":300,"elapsed":142096,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"ad2c2844-8c92-45f0-cd36-3c4153526e18"},"source":["# Create 10 sentences using our language model\r\n","# the sentences will be max_length long or when the model\r\n","# predicts EOS token.\r\n","for _ in range(10):\r\n","    print(prediction(max_length, ''))"],"execution_count":46,"outputs":[{"output_type":"stream","text":["\"already?\" asked vicarage.\n","me without your measure.\"\n","i had taken abar that the game he need, to some tongania, whatever\n","raised streets, and i saw a prompt fellow which was broken out,\n","there any trace of the man.\"\n","it outside the room, sooner which had happened.\n","and round the dooches, something in the state which had been so\n","\"but it was easy. what frequent?\"\n","\"have you arranged silverton extended figures at the fall, holmes,\n","business, and my pipe is so close towards me on one side, as was\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ZYrCg5YmVx1W","executionInfo":{"status":"ok","timestamp":1607635694769,"user_tz":300,"elapsed":51752,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":[""],"execution_count":48,"outputs":[]}]}