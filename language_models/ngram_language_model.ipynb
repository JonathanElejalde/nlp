{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ngram_language_model.ipynb","provenance":[],"mount_file_id":"1xZokeORwfxnvaeA_N8kgKZXQ9eBeGY0J","authorship_tag":"ABX9TyMLNzgHvUftH9Kl+DH0EFpz"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4BNSa2BZykeA","executionInfo":{"status":"ok","timestamp":1607444698999,"user_tz":300,"elapsed":1343,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"e754a159-b0a5-4776-acb8-d8a9dae05ae2"},"source":["%cd /content/drive/MyDrive/Colab Notebooks/nlp/apps"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/nlp/apps\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RfQKAwany2KX","executionInfo":{"status":"ok","timestamp":1607444699002,"user_tz":300,"elapsed":1339,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["# This is just the first novel, we use it for testing purposes because it is smaller\n","testing_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/study in scarlet.txt'\n","\n","# This is the whole corpus\n","path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/sherlock_novels.txt'"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jGDtwcxnzAX1"},"source":["# Preprocessing the corpus\n","\n","These are the preprocessing steps that we are going to use:\n","\n","- lowercase the text\n","- remove special characters\n","- split text to list of sentences\n","- split sentences into list of words\n","\n","Notice that we will consider each line as a sentences for this language model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yGp6_0EwzcVk","executionInfo":{"status":"ok","timestamp":1607444701175,"user_tz":300,"elapsed":3500,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"36c7b4a8-38aa-45ce-d58b-937eb1cb5fcd"},"source":["import nltk\n","import re\n","import numpy as np\n","import pandas as pd\n","\n","nltk.download('punkt')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"WtqGETKzzfiF","executionInfo":{"status":"ok","timestamp":1607444701179,"user_tz":300,"elapsed":3499,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def remove_special(sentence):\n","    \"\"\"\n","    Takes a sentence and only keeps .,?! and space\n","    as special characters.\n","    Args:\n","        sentence: str\n","    returns\n","        sentence: str. The full sentence cleaned of special characters\n","    \"\"\"\n","    sentence = re.sub(r'[^a-zA-Z0-9.,?! ]+', '', sentence)\n","\n","    return sentence\n","\n","def get_text(path):\n","    \"\"\"\n","    It reads a txt file and returns a string with all the corpus\n","    Args:\n","        path: str\n","    returns:\n","        text: str\n","    \"\"\"\n","    with open(path) as f:\n","        text = f.read()\n","\n","    return text\n","\n","def get_sentences(text):\n","    \"\"\"\n","    Takes a whole text removes special characters and divides it by \\n\n","    then it returns a list of list with the sentences\n","    Args:\n","        text: str\n","    returns:\n","        sentences: list\n","    \"\"\"\n","    text = text.lower()\n","    sentences = text.split('\\n')\n","    # also removes any empty line\n","    sentences = [remove_special(sentence.strip()) for sentence in sentences if len(sentence) > 0]\n","\n","    return sentences\n","\n","# Uncomment to see the 10 first sentences\n","\n","# text = get_text(testing_path)\n","# sentences = get_sentences(text)\n","# for s in sentences[:10]:\n","#     print(s)\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3k1oWSNSSYeN"},"source":["# Tokenize the corpus"]},{"cell_type":"code","metadata":{"id":"u612FuiD4KrQ","executionInfo":{"status":"ok","timestamp":1607444701183,"user_tz":300,"elapsed":3500,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def tokenize(sentences):\n","    \"\"\"\n","    It takes a list of strings that are the sentences\n","    and returns a list of list of tokens\n","    Args:\n","        sentences: list\n","    returns:\n","        tokenized_sentences: list\n","    \"\"\"\n","    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n","\n","    return tokenized_sentences\n","\n","# Uncomment to test\n","\n","# text = get_text(testing_path)\n","# sentences = get_sentences(text)\n","# tokenized_sentences = tokenize(sentences[:10])\n","# for s in tokenized_sentences:\n","#     print(s)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"j-CgKRo07Ex6","executionInfo":{"status":"ok","timestamp":1607444701185,"user_tz":300,"elapsed":3499,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def get_tokens(path):\n","    \"\"\"\n","    It takes the path of a txt file and applies \n","    get_text(), get_sentences(), and tokenize()\n","    functions .\n","    Args:\n","        path: str\n","    returns:\n","        tokenized_sentences: list\n","    \"\"\"\n","    text = get_text(path)\n","    sentences = get_sentences(text)\n","    tokenized_sentences = tokenize(sentences)\n","\n","    return tokenized_sentences\n","    "],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k6SOLbYz-Ek8"},"source":["# Count words\n","\n","We are going to pass through each sentence and each token counting each tokens occurrence in the corpus.\n","\n","This will help us to take the tokens that appear N times in the corpus and also to calculate probabilities"]},{"cell_type":"code","metadata":{"id":"28_b__CXHmsx","executionInfo":{"status":"ok","timestamp":1607444701187,"user_tz":300,"elapsed":3498,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def get_token_counts(tokenized_sentences):\n","    \"\"\"\n","    It takes a list of list of tokens and returns\n","    a dict where the key are going to be the tokens \n","    and the value is how many times it appears\n","    Args:\n","        tokenized_sentences: list\n","    returns:\n","        token_counts: dict\n","    \"\"\"\n","    token_counts = dict()\n","    for sentence in tokenized_sentences:\n","        for token in sentence:\n","            if token not in token_counts.keys():\n","                token_counts[token] = 1\n","            else:\n","                token_counts[token] += 1\n","    \n","    return token_counts\n","\n","# Uncomment for testing\n","\n","# to = get_tokens(testing_path)\n","# counts = get_token_counts(to)\n","# from collections import Counter\n","# c = Counter(counts)\n","# c.most_common(10)     "],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_XxS_wJ1JHjh"},"source":["# Handling out of vocabulary words\n","\n","Because it is probable that in some point we are going to encounter words that were not in our training dataset, we need to handle out of vocabulary words. Otherwise, we won't be able to predict the next word.\n","\n","in this case, we are going to add an \"unk\" token, which is going to replace the words with less than N occurrences in the training data and the words left are going to be our vocab.\n"]},{"cell_type":"code","metadata":{"id":"x4h3UUcWJ_uV","executionInfo":{"status":"ok","timestamp":1607444701189,"user_tz":300,"elapsed":3496,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["threshold = 2\n","\n","def add_unk_token(tokenized_sentences, vocab, unk_token):\n","    \"\"\"\n","    It updates the tokens that are not in the vocab\n","    to the unk token\n","    Args:\n","        tokenized_sentences: list\n","        vocab: set\n","        unk_token: str\n","    returns:\n","        tokenized_sentences_with_unk: list. updated list of list of tokens with\n","            the unk character\n","    \"\"\"\n","\n","    tokenized_sentences_with_unk = []\n","\n","    for sentence in tokenized_sentences:\n","        # we need to keep track of the new sentence\n","        new_sentence = []\n","\n","        for token in sentence:\n","            if token in vocab:\n","                new_sentence.append(token)\n","            else:\n","                new_sentence.append(unk_token)\n","        \n","        # save the new sentence\n","        tokenized_sentences_with_unk.append(new_sentence)\n","\n","    return tokenized_sentences_with_unk\n","\n","def create_new_sentences(tokenized_sentences, mode='train', vocab=None, threshold=2, unk_token='unk'):\n","    \"\"\"\n","    It takes a list of list of tokens, counts the tokens occurrences and\n","    search for the tokens with less occurrences than the threshold. Then it\n","    transform them into the \"unk\" token.\n","    Args:\n","        tokenized_sentences: list\n","        mode: str. (train or test)\n","        vocab: list. (we get the vocab from train and use it again for test)\n","        threshold: int\n","        unk_token: str\n","    returns:\n","        tokenized_sentences_with_unk: dict. Updated with the unk token\n","        if mode == 'train'\n","            vocab: set\n","    \"\"\"\n","    if mode == 'train':\n","        vocab = []\n","        token_counts = get_token_counts(tokenized_sentences)\n","\n","        for word, count in token_counts.items():\n","            # check the threshold\n","            if count >= threshold:\n","                vocab.append(word)\n","        \n","        # cast the vocab to set. It will allow faster search\n","        vocab = set(vocab)\n","\n","        tokenized_sentences_with_unk = add_unk_token(tokenized_sentences, vocab, unk_token)\n","\n","        return tokenized_sentences_with_unk, vocab\n","\n","    elif mode == 'test':\n","        vocab = vocab\n","        tokenized_sentences_with_unk = add_unk_token(tokenized_sentences, vocab, unk_token)\n","        \n","        return tokenized_sentences_with_unk\n","\n","    else:\n","        raise Exception(\"Wrong mode was passed\") \n","\n","\n","# Uncomment for testing\n","\n","# tokens = get_tokens(testing_path)\n","# updated_tokens, vocab = create_new_sentences(tokens, threshold=threshold)\n","# from collections import Counter\n","# token_counts = get_token_counts(updated_tokens)\n","# c = Counter(token_counts)\n","# print(c['unk'])\n","# c.most_common(10)\n"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"snvxOKgv0eC5"},"source":["# Create the corpus and split into train and test sets"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"raoNyKON8IZ3","executionInfo":{"status":"ok","timestamp":1607444702886,"user_tz":300,"elapsed":5186,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"9c4491bf-07f9-4ebb-fb64-afaf0b924d5f"},"source":["import random\n","\n","tokenized_sentences = get_tokens(testing_path)\n","random.seed(10)\n","random.shuffle(tokenized_sentences)\n","print(f'Amount of sentences {len(tokenized_sentences)}')"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Amount of sentences 3924\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wqGdvjl08idO"},"source":["### Because the corpus is big enough we can test using just 10% of the sentences"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lhNZjT2a9knc","executionInfo":{"status":"ok","timestamp":1607444702889,"user_tz":300,"elapsed":5182,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"6b6b071b-f467-4ff5-fb87-043ed8c1e239"},"source":["size = int(len(tokenized_sentences) * 0.9)\n","train = tokenized_sentences[:size]\n","test = tokenized_sentences[size:]\n","print(f'Training size: {len(train)}')\n","print(f'Testing size: {len(test)}')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Training size: 3531\n","Testing size: 393\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"30xDrlSUQFf5"},"source":["# Preprocess the data\n","\n","In this step, we are going to join the functions that we have been creating to process our train and test datasets."]},{"cell_type":"code","metadata":{"id":"10yl5i-voNaU","executionInfo":{"status":"ok","timestamp":1607444702894,"user_tz":300,"elapsed":5184,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def preprocess(train, test, threshold=2):\n","    \"\"\"\n","    It takes the train and test datasets (list of list of tokens)\n","    and preprocesses them. We will end with a train and test datasets\n","    updated with the unk token and the vocab.\n","    Args:\n","        train: list\n","        test: list\n","        threshold: int\n","    returns:\n","        train_sentences: list\n","        test_sentences: list\n","        vocab: set\n","    \"\"\"\n","    train_sentences, vocab = create_new_sentences(train, threshold=threshold)\n","    test_sentences = create_new_sentences(test, mode='test', vocab=vocab)\n","\n","    return train_sentences, test_sentences, vocab\n","\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"DLQFF9cwwF6B","executionInfo":{"status":"ok","timestamp":1607444702898,"user_tz":300,"elapsed":5185,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["train_sentences, test_sentences, vocab = preprocess(train, test, threshold=threshold)"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7yzut72lybfR"},"source":["# N-gram model\n","\n","The model is going to predict the next word based on the previous n-gram. The model is going to use the conditional probability of a word appearing after a serie of words. So, \n","\n","- The numerator is the number of times that the word appears after the serie of previous words.\n","- The denominator is the number of times that this serie or words appears in the training data.\n","\n","also, to be able to know thr probabilities of a word in the beginning of a sentences and the end we have to add two tokens.\n","\n","Depending on the number of words that we are going to use for the n-gram we have to add n - 1 tokens at the beginning and always one at the end.\n"]},{"cell_type":"code","metadata":{"id":"5LX-pLyJBgfd","executionInfo":{"status":"ok","timestamp":1607444702900,"user_tz":300,"elapsed":5184,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def count_ngrams(tokenized_sentences, n, start_token='<s>', end_token='<e>'):\n","    \"\"\"\n","    it takes a list of list of tokens and counts all the possible\n","    n-grams and returns the counts for each n-gram as a dict.\n","    Args:\n","        tokenized_sentences: list (list of tokenized sentences)\n","        n: int\n","        start_token: str\n","        end_token: str\n","    returns:\n","        ngram_count: dict\n","    \"\"\"\n","    ngram_count = dict()\n","\n","    for sentence in tokenized_sentences:\n","        # add the start_tokens and end_token to each sentence\n","        sentence = [start_token] * n + sentence + [end_token]\n","\n","        # Cast the sentence to a tuple, thus we can use it as\n","        # a key for the dict\n","        sentence = tuple(sentence)\n","\n","        for i in range(len(sentence) - n + 1):\n","            ngram = tuple(sentence[i:i+n])\n","            if ngram in ngram_count.keys():\n","                ngram_count[ngram] += 1\n","            else:\n","                ngram_count[ngram] = 1\n","    \n","    return ngram_count\n","\n","\n"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5kTtt6cpzpMI"},"source":["# Computing probabilities for all the words in the data\n"]},{"cell_type":"code","metadata":{"id":"L6HCIg2YBYSW","executionInfo":{"status":"ok","timestamp":1607444702901,"user_tz":300,"elapsed":5182,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def create_matrix(nplus1_gram_counts, vocab):\n","    \"\"\"\n","    It creates a matrix where the columns are the words in the vocabulary and\n","    the rows are the ngram previous to the target word.\n","    \n","    Args:\n","        nplus1_gram_counts: dict\n","        vocab: list\n","    returns:\n","        count_matrix: Pandas DataFrame\n","    \"\"\"\n","    vocab = list(vocab) + ['<e>', 'unk']\n","\n","    # obtain unique ngrams\n","    ngrams = []\n","    for ngram_plus1 in nplus1_gram_counts.keys():\n","        ngram = ngram_plus1[:-1]\n","        ngrams.append(ngram)\n","    \n","    \n","    # eliminate duplicates\n","    ngrams = list(set(ngrams))\n","\n","    row_index = {ngram:i for i, ngram in enumerate(ngrams)}\n","    col_index = {word:i for i, word in enumerate(vocab)}\n","\n","    nrow = len(ngrams)\n","    ncol = len(vocab)\n","    matrix = np.zeros((nrow, ncol))\n","\n","    for ngram_plus1, count in nplus1_gram_counts.items():\n","        ngram = ngram_plus1[:-1]\n","        word = ngram_plus1[-1]\n","\n","        if word not in vocab:\n","            continue\n","        i = row_index[ngram]\n","        j = col_index[word]\n","        matrix[i, j] = count\n","\n","    count_matrix = pd.DataFrame(matrix, index=ngrams, columns=vocab)\n","\n","    return count_matrix\n","\n","def create_probability_matrix(count_matrix, k):\n","    \"\"\"\n","    It adds smoothing to the values in the count_matrix to avoid dividing by zero\n","    error, then it calculates the probability of each word appearing after\n","    a previous series of words (ngrams)\n","\n","    Args:\n","        count_matrix: pandas DataFrame\n","        k: int\n","    returns:\n","        probability_matrix: pandas DataFrame\n","    \"\"\"\n","    count_matrix += k\n","    probability_matrix = count_matrix.div(count_matrix.sum(axis=1), axis=0)\n","\n","    return probability_matrix\n","\n","\n"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XIWrFu_vMZR1"},"source":["# Perplexity\n","\n","This is what we are going to use as our evaluation metric:\n","\n","$$ PP(W) =\\sqrt[N]{ \\prod_{t=n}^{N-1} \\frac{1}{P(w_t | w_{t-n} \\cdots w_{t-1})} }$$\n","\n"]},{"cell_type":"code","metadata":{"id":"vcmlU4pqQVVd","executionInfo":{"status":"ok","timestamp":1607446059380,"user_tz":300,"elapsed":863,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def compute_word_probability(word, previous_ngram, ngram_counts, nplus1_gram_counts, vocab_size, k=1.0):\n","    \"\"\"\n","    Computes the probability of a next word using the ngram_counts and laplacian smoothing\n","    Args:\n","        word: str\n","        previous_ngram: tuple\n","        ngram_counts: dict\n","        nplus1_gram_counts: dict\n","        vocab_size: int\n","        k: float\n","    returns:\n","        probability: float\n","    \"\"\"\n","    # previous_ngram should be a tuple\n","    previous_ngram = tuple(previous_ngram)\n","\n","    # look for the previous ngram count in the dict\n","    # get the count, if it does not exist set it to 0\n","    previous_ngram_count = ngram_counts.get(previous_ngram, 0)\n","    denominator = previous_ngram_count + k * vocab_size\n","\n","    # the nplus1_gram is the previous_ngram + the word we are getting the proba\n","    nplus1_gram = tuple(previous_ngram + (word,))\n","\n","    # check for the count in the nplus1_gram_counts dict, if not set it to 0\n","    nplus1_gram_count = nplus1_gram_counts.get(nplus1_gram, 0)\n","    numerator = nplus1_gram_count + k\n","\n","    # compute probability\n","    probability = numerator / denominator\n","\n","    return probability\n","\n","def compute_perplexity(sentence, ngram_counts, nplus1_gram_counts, vocab_size, k=1.0):\n","    # lenght of previous words\n","    n = len(list(ngram_counts.keys())[0])\n","\n","    # add <s> and <e>\n","    sentence = ['<s>'] * n + sentence + ['<e>']\n","    sentence = tuple(sentence)\n","\n","    # lenght of sentence after adding tokens\n","    N = len(sentence)\n","\n","    product_pi = 1.0\n","\n","    for i in range(n, N):\n","        # get ngram preceding the word in position i\n","        ngram = sentence[i - n:i]\n","\n","        # get word at position i\n","        word = sentence[i]\n","\n","        # compute the probability of the word given the ngram\n","        # using the ngram counts, nplus1 gram counts,\n","        # vocab size, and smoothing constant\n","        probability = compute_word_probability(word, ngram, ngram_counts, nplus1_gram_counts, vocab_size)\n","\n","        product_pi *= 1 / probability\n","\n","    perplexity = product_pi ** (1/float(N))\n","\n","    return perplexity\n","\n","    "],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"rIRyzVoYP_yt"},"source":[""],"execution_count":null,"outputs":[]}]}