{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ngram_language_model.ipynb","provenance":[],"mount_file_id":"1xZokeORwfxnvaeA_N8kgKZXQ9eBeGY0J","authorship_tag":"ABX9TyMwdJ116swbl+h8vezNN0Ja"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4BNSa2BZykeA","executionInfo":{"status":"ok","timestamp":1607370151497,"user_tz":300,"elapsed":679,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"466d2782-0939-4ce7-f36c-404febac91e3"},"source":["%cd /content/drive/MyDrive/Colab Notebooks/nlp/apps"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/nlp/apps\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RfQKAwany2KX","executionInfo":{"status":"ok","timestamp":1607370151799,"user_tz":300,"elapsed":968,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["# This is just the first novel, we use it for testing purposes because it is smaller\n","testing_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/study in scarlet.txt'\n","\n","# This is the whole corpus\n","path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/sherlock_novels.txt'"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jGDtwcxnzAX1"},"source":["# Preprocessing the corpus\n","\n","These are the preprocessing steps that we are going to use:\n","\n","- lowercase the text\n","- remove special characters\n","- split text to list of sentences\n","- split sentences into list of words\n","\n","Notice that we will consider each line as a sentences for this language model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yGp6_0EwzcVk","executionInfo":{"status":"ok","timestamp":1607370152705,"user_tz":300,"elapsed":1865,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"63988616-729a-49d1-db31-c2417fd77206"},"source":["import nltk\n","import re\n","\n","nltk.download('punkt')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"WtqGETKzzfiF","executionInfo":{"status":"ok","timestamp":1607370152708,"user_tz":300,"elapsed":1862,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def remove_special(sentence):\n","    \"\"\"\n","    Takes a sentence and only keeps .,?! and space\n","    as special characters.\n","    Args:\n","        sentence: str\n","    returns\n","        sentence: str. The full sentence cleaned of special characters\n","    \"\"\"\n","    sentence = re.sub(r'[^a-zA-Z0-9.,?! ]+', '', sentence)\n","\n","    return sentence\n","\n","def get_text(path):\n","    \"\"\"\n","    It reads a txt file and returns a string with all the corpus\n","    Args:\n","        path: str\n","    returns:\n","        text: str\n","    \"\"\"\n","    with open(path) as f:\n","        text = f.read()\n","\n","    return text\n","\n","def get_sentences(text):\n","    \"\"\"\n","    Takes a whole text removes special characters and divides it by \\n\n","    then it returns a list of list with the sentences\n","    Args:\n","        text: str\n","    returns:\n","        sentences: list\n","    \"\"\"\n","    text = text.lower()\n","    sentences = text.split('\\n')\n","    # also removes any empty line\n","    sentences = [remove_special(sentence.strip()) for sentence in sentences if len(sentence) > 0]\n","\n","    return sentences\n","\n","# Uncomment to see the 10 first sentences\n","\n","# text = get_text(testing_path)\n","# sentences = get_sentences(text)\n","# for s in sentences[:10]:\n","#     print(s)\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3k1oWSNSSYeN"},"source":["# Tokenize the corpus"]},{"cell_type":"code","metadata":{"id":"u612FuiD4KrQ","executionInfo":{"status":"ok","timestamp":1607370152713,"user_tz":300,"elapsed":1862,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def tokenize(sentences):\n","    \"\"\"\n","    It takes a list of strings that are the sentences\n","    and returns a list of list of tokens\n","    Args:\n","        sentences: list\n","    returns:\n","        tokenized_sentences: list\n","    \"\"\"\n","    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n","\n","    return tokenized_sentences\n","\n","# Uncomment to test\n","\n","# text = get_text(testing_path)\n","# sentences = get_sentences(text)\n","# tokenized_sentences = tokenize(sentences[:10])\n","# for s in tokenized_sentences:\n","#     print(s)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4t-iQ_7e5oHd"},"source":["# Create training and test datsets"]},{"cell_type":"code","metadata":{"id":"j-CgKRo07Ex6","executionInfo":{"status":"ok","timestamp":1607370161505,"user_tz":300,"elapsed":10649,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["import random\n","\n","# First create a function to get the tokens\n","def get_tokens(path):\n","    \"\"\"\n","    It takes the path of a txt file and applies \n","    get_text(), get_sentences(), and tokenize()\n","    functions .\n","    Args:\n","        path: str\n","    returns:\n","        tokenized_sentences: list\n","    \"\"\"\n","    text = get_text(path)\n","    sentences = get_sentences(text)\n","    tokenized_sentences = tokenize(sentences)\n","\n","    return tokenized_sentences\n","\n","tokenized_sentences = get_tokens(path)\n","    "],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"raoNyKON8IZ3","executionInfo":{"status":"ok","timestamp":1607370161851,"user_tz":300,"elapsed":10986,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"85132df4-64fa-4d37-fd70-7ebbc1d0de33"},"source":["random.seed(10)\n","random.shuffle(tokenized_sentences)\n","print(f'Amount of sentences {len(tokenized_sentences)}')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Amount of sentences 60198\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wqGdvjl08idO"},"source":["### Because the corpus is big enough we can test using just 10% of the sentences"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lhNZjT2a9knc","executionInfo":{"status":"ok","timestamp":1607370161855,"user_tz":300,"elapsed":10981,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"26dd6d81-c717-4d2d-edd5-d0b9ae93fad2"},"source":["size = int(len(tokenized_sentences) * 0.9)\n","train = tokenized_sentences[:size]\n","test = tokenized_sentences[size:]\n","print(f'Training size: {len(train)}')\n","print(f'Testing size: {len(test)}')"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Training size: 54178\n","Testing size: 6020\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"k6SOLbYz-Ek8"},"source":["# Count words\n","\n","We are going to pass through each sentence and each token counting each tokens occurrence in the corpus.\n","\n","This will help us to take the tokens that appear N times in the corpus and also to calculate probabilities"]},{"cell_type":"code","metadata":{"id":"28_b__CXHmsx","executionInfo":{"status":"ok","timestamp":1607370161859,"user_tz":300,"elapsed":10978,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def get_token_counts(tokenized_sentences):\n","    \"\"\"\n","    It takes a list of list of tokens and returns\n","    a dict where the key are going to be the tokens \n","    and the value is how many times it appears\n","    Args:\n","        tokenized_sentences: list\n","    returns:\n","        token_counts: dict\n","    \"\"\"\n","    token_counts = dict()\n","    for sentence in tokenized_sentences:\n","        for token in sentence:\n","            if token not in token_counts.keys():\n","                token_counts[token] = 1\n","            else:\n","                token_counts[token] += 1\n","    \n","    return token_counts\n","\n","# Uncomment for testing\n","\n","# to = get_tokens(testing_path)\n","# counts = get_token_counts(to)\n","# from collections import Counter\n","# c = Counter(counts)\n","# c.most_common(10)     "],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_XxS_wJ1JHjh"},"source":["# Handling out of vocabulary words\n","\n","Because it is probable that in some point we are going to encounter words that were not in our training dataset, we need to handle out of vocabulary words. Otherwise, we won't be able to predict the next word.\n","\n","in this case, we are going to add an \"unk\" token, which is going to replace the words with less than N occurrences in the training data and the words left are going to be our vocab.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x4h3UUcWJ_uV","executionInfo":{"status":"ok","timestamp":1607370319506,"user_tz":300,"elapsed":1616,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"a05972c8-212c-4b76-8750-3db28385d1a0"},"source":["threshold = 2\n","\n","def add_unk_token(tokenized_sentences, vocab, unk_token):\n","    \"\"\"\n","    It updates the tokens that are not in the vocab\n","    to the unk token\n","    Args:\n","        tokenized_sentences: list\n","        vocab: set\n","        unk_token: str\n","    returns:\n","        tokenized_sentences_with_unk: list. updated list of list of tokens with\n","            the unk character\n","    \"\"\"\n","\n","    tokenized_sentences_with_unk = []\n","\n","    for sentence in tokenized_sentences:\n","        # we need to keep track of the new sentence\n","        new_sentence = []\n","\n","        for token in sentence:\n","            if token in vocab:\n","                new_sentence.append(token)\n","            else:\n","                new_sentence.append(unk_token)\n","        \n","        # save the new sentence\n","        tokenized_sentences_with_unk.append(new_sentence)\n","\n","    return tokenized_sentences_with_unk\n","\n","def create_new_sentences(tokenized_sentences, mode='train', vocab=None, threshold=2, unk_token='unk'):\n","    \"\"\"\n","    It takes a list of list of tokens, counts the tokens occurrences and\n","    search for the tokens with less occurrences than the threshold. Then it\n","    transform them into the \"unk\" token.\n","    Args:\n","        tokenized_sentences: list\n","        mode: str. (train or test)\n","        vocab: list. (we get the vocab from train and use it again for test)\n","        threshold: int\n","        unk_token: str\n","    returns:\n","        tokenized_sentences_with_unk: dict. Updated with the unk token\n","        if mode == 'train'\n","            vocab: set\n","    \"\"\"\n","    if mode == 'train':\n","        vocab = []\n","        token_counts = get_token_counts(tokenized_sentences)\n","\n","        for word, count in token_counts.items():\n","            # check the threshold\n","            if count >= threshold:\n","                vocab.append(word)\n","        \n","        # cast the vocab to set. It will allow faster search\n","        vocab = set(vocab)\n","\n","        tokenized_sentences_with_unk = add_unk_token(tokenized_sentences, vocab, unk_token)\n","\n","        return tokenized_sentences_with_unk, vocab\n","\n","    elif mode == 'test':\n","        vocab = vocab\n","        tokenized_sentences_with_unk = add_unk_token(tokenized_sentences, vocab, unk_token)\n","        \n","        return tokenized_sentences_with_unk\n","\n","    else:\n","        assert mode not in ['train', 'test'], \"Wrong mode. options = train or test\"\n","\n","# Uncomment for testing\n","\n","tokens = get_tokens(testing_path)\n","updated_tokens, vocab = create_new_sentences(tokens, threshold=threshold)\n","from collections import Counter\n","token_counts = get_token_counts(updated_tokens)\n","c = Counter(token_counts)\n","print(f'----Training-----')\n","print(c['unk'])\n","c.most_common(10)\n"],"execution_count":13,"outputs":[{"output_type":"stream","text":["3239\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[('unk', 3239),\n"," (',', 2953),\n"," ('the', 2521),\n"," ('.', 2383),\n"," ('and', 1348),\n"," ('of', 1206),\n"," ('to', 1082),\n"," ('a', 990),\n"," ('i', 895),\n"," ('he', 794)]"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"30xDrlSUQFf5"},"source":["# Preprocess the data\n","\n","In this step, we are going to join the functions that we have been creating to process our train and test datasets."]},{"cell_type":"code","metadata":{"id":"10yl5i-voNaU","executionInfo":{"status":"aborted","timestamp":1607370162615,"user_tz":300,"elapsed":11720,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def preprocess(train, test, mode='train', threshold=2):\n","    \"\"\"\n","    It takes the train and test datasets (list of list of tokens)\n","    and preprocesses them. We will end with a train and test datasets\n","    updated with the unk token and the vocab.\n","    Args:\n","        train: list\n","        test: list\n","        threshold: int\n","    returns:\n","        train_sentences: list\n","        test_sentences: list\n","        vocab: set\n","    \"\"\"\n","    pass\n","\n"],"execution_count":null,"outputs":[]}]}