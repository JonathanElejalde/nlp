{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ngram_language_model.ipynb","provenance":[],"mount_file_id":"1xZokeORwfxnvaeA_N8kgKZXQ9eBeGY0J","authorship_tag":"ABX9TyO69RDoWLlNtXQ9h7Rcgud0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4BNSa2BZykeA","executionInfo":{"status":"ok","timestamp":1607357188446,"user_tz":300,"elapsed":705,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"3246ad44-cb04-4e3b-9209-03d4f26775cd"},"source":["%cd /content/drive/MyDrive/Colab Notebooks/nlp/apps"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/nlp/apps\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RfQKAwany2KX","executionInfo":{"status":"ok","timestamp":1607357188805,"user_tz":300,"elapsed":1051,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["# This is just the first novel, we use it for testing purposes because it is smaller\n","testing_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/study in scarlet.txt'\n","\n","# This is the whole corpus\n","path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/sherlock_novels.txt'"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jGDtwcxnzAX1"},"source":["# Preprocessing the corpus\n","\n","These are the preprocessing steps that we are going to use:\n","\n","- lowercase the text\n","- remove special characters\n","- split text to list of sentences\n","- split sentences into list of words\n","\n","Notice that we will consider each line as a sentences for this language model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yGp6_0EwzcVk","executionInfo":{"status":"ok","timestamp":1607357190070,"user_tz":300,"elapsed":2308,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"f9b2bf60-9420-4326-affc-684aed741450"},"source":["import nltk\n","import re\n","\n","nltk.download('punkt')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"WtqGETKzzfiF","executionInfo":{"status":"ok","timestamp":1607357190072,"user_tz":300,"elapsed":2302,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def remove_special(sentence):\n","    \"\"\"\n","    Takes a sentence and only keeps .,?! and space\n","    as special characters.\n","    Args:\n","        sentence: str\n","    returns\n","        sentence: str. The full sentence cleaned of special characters\n","    \"\"\"\n","    sentence = re.sub(r'[^a-zA-Z0-9.,?! ]+', '', sentence)\n","\n","    return sentence\n","\n","def get_text(path):\n","    \"\"\"\n","    It reads a txt file and returns a string with all the corpus\n","    Args:\n","        path: str\n","    returns:\n","        text: str\n","    \"\"\"\n","    with open(path) as f:\n","        text = f.read()\n","\n","    return text\n","\n","def get_sentences(text):\n","    \"\"\"\n","    Takes a whole text removes special characters and divides it by \\n\n","    then it returns a list of list with the sentences\n","    Args:\n","        text: str\n","    returns:\n","        sentences: list\n","    \"\"\"\n","    text = text.lower()\n","    sentences = text.split('\\n')\n","    # also removes any empty line\n","    sentences = [remove_special(sentence.strip()) for sentence in sentences if len(sentence) > 0]\n","\n","    return sentences\n","\n","# Uncomment to see the 10 first sentences\n","\n","# text = get_text(testing_path)\n","# sentences = get_sentences(text)\n","# for s in sentences[:10]:\n","#     print(s)\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3k1oWSNSSYeN"},"source":["# Tokenize the corpus"]},{"cell_type":"code","metadata":{"id":"u612FuiD4KrQ","executionInfo":{"status":"ok","timestamp":1607357190075,"user_tz":300,"elapsed":2301,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def tokenize(sentences):\n","    \"\"\"\n","    It takes a list of strings that are the sentences\n","    and returns a list of list of tokens\n","    Args:\n","        sentences: list\n","    returns:\n","        tokenized_sentences: list\n","    \"\"\"\n","    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n","\n","    return tokenized_sentences\n","\n","# Uncomment to test\n","\n","# text = get_text(testing_path)\n","# sentences = get_sentences(text)\n","# tokenized_sentences = tokenize(sentences[:10])\n","# for s in tokenized_sentences:\n","#     print(s)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4t-iQ_7e5oHd"},"source":["# Create training and test datsets"]},{"cell_type":"code","metadata":{"id":"j-CgKRo07Ex6","executionInfo":{"status":"ok","timestamp":1607357209242,"user_tz":300,"elapsed":21461,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["import random\n","\n","# First create a function to get the tokens\n","def get_tokens(path):\n","    \"\"\"\n","    It takes the path of a txt file and applies \n","    get_text(), get_sentences(), and tokenize()\n","    functions .\n","    Args:\n","        path: str\n","    returns:\n","        tokenized_sentences: list\n","    \"\"\"\n","    text = get_text(path)\n","    sentences = get_sentences(text)\n","    tokenized_sentences = tokenize(sentences)\n","\n","    return tokenized_sentences\n","\n","tokenized_sentences = get_tokens(path)\n","    "],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"raoNyKON8IZ3","executionInfo":{"status":"ok","timestamp":1607357209245,"user_tz":300,"elapsed":21459,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"1ead457b-910b-4428-df0e-e645f8d030b9"},"source":["random.seed(10)\n","random.shuffle(tokenized_sentences)\n","print(f'Amount of sentences {len(tokenized_sentences)}')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Amount of sentences 60198\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wqGdvjl08idO"},"source":["### Because the corpus is big enough we can test using just 10% of the sentences"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lhNZjT2a9knc","executionInfo":{"status":"ok","timestamp":1607357519499,"user_tz":300,"elapsed":1203,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"5f0c3b02-df01-4bd6-ccb0-1c7ec3106f8b"},"source":["size = int(len(tokenized_sentences) * 0.9)\n","train = tokenized_sentences[:size]\n","test = tokenized_sentences[size:]\n","print(f'Training size: {len(train)}')\n","print(f'Testing size: {len(test)}')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Training size: 54178\n","Testing size: 6020\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"k6SOLbYz-Ek8"},"source":["# Count words\n","\n","We are going to pass through each sentence and each token counting each tokens occurrence in the corpus.\n","\n","This will help us to take the tokens that appear N times in the corpus and also to calculate probabilities"]},{"cell_type":"code","metadata":{"id":"28_b__CXHmsx","executionInfo":{"status":"ok","timestamp":1607361865712,"user_tz":300,"elapsed":1059,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def get_token_counts(tokenized_sentences):\n","    \"\"\"\n","    It takes a list of list of tokens and returns\n","    a dict where the key are going to be the tokens \n","    and the value is how many times it appears\n","    Args:\n","        tokenized_sentences: list\n","    returns:\n","        token_counts: dict\n","    \"\"\"\n","    token_counts = dict()\n","    for sentence in tokenized_sentences:\n","        for token in sentence:\n","            if token not in token_counts.keys():\n","                token_counts[token] = 1\n","            else:\n","                token_counts[token] += 1\n","    \n","    return token_counts\n","\n","# Uncomment for testing\n","\n","# to = get_tokens(testing_path)\n","# counts = get_token_counts(to)\n","# from collections import Counter\n","# c = Counter(counts)\n","# c.most_common(10)     "],"execution_count":19,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_XxS_wJ1JHjh"},"source":["# Handling out of vocabulary words\n","\n","Because it is probable that in some point we are going to encounter words that were not in our training dataset, we need to handle out of vocabulary words. Otherwise, we won't be able to predict the next word.\n","\n","in this case, we are going to add an \"unk\" token, which is going to replace the words with less than N occurrences in the training data and the words left are going to be our vocab.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x4h3UUcWJ_uV","executionInfo":{"status":"ok","timestamp":1607362090156,"user_tz":300,"elapsed":1732,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"4f2415bc-5ffe-45e5-e50f-b48438213646"},"source":["threshold = 2\n","\n","def create_new_sentences(tokenized_sentences, threshold=1, unk_token='unk'):\n","    \"\"\"\n","    It takes a list of list of tokens, counts the tokens occurrences and\n","    search for the tokens with less occurrences than the threshold. Then it\n","    transform them into the \"unk\" token.\n","    Args:\n","        tokenized_sentences: list\n","        threshold: int\n","    returns:\n","        tokenized_sentences_with_unk: dict. Updated with the unk token\n","    \"\"\"\n","    vocab = []\n","    token_counts = get_token_counts(tokenized_sentences)\n","\n","    for word, count in token_counts.items():\n","        # check the threshold\n","        if count >= threshold:\n","            vocab.append(word)\n","    \n","    # cast the vocab to set. It will allow faster search\n","    vocab = set(vocab)\n","\n","    # now, the words that are not in the vocab will\n","    # be changed to the unk token\n","\n","    tokenized_sentences_with_unk = []\n","\n","    for sentence in tokenized_sentences:\n","        # we need to keep track of the new sentence\n","        new_sentence = []\n","\n","        for token in sentence:\n","            if token in vocab:\n","                new_sentence.append(token)\n","            else:\n","                new_sentence.append(unk_token)\n","        \n","        # save the new sentence\n","        tokenized_sentences_with_unk.append(new_sentence)\n","\n","    return tokenized_sentences_with_unk\n","\n","# Uncomment for testing\n","\n","# to = get_tokens(testing_path)\n","# new_s = create_new_sentences(to, threshold)\n","# new_count = get_token_counts(new_s)\n","# from collections import Counter\n","# c = Counter(new_count)\n","# print(c['unk'])\n","# c.most_common(10)\n","\n","        \n","\n"],"execution_count":30,"outputs":[{"output_type":"stream","text":["3239\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[('unk', 3239),\n"," (',', 2953),\n"," ('the', 2521),\n"," ('.', 2383),\n"," ('and', 1348),\n"," ('of', 1206),\n"," ('to', 1082),\n"," ('a', 990),\n"," ('i', 895),\n"," ('he', 794)]"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"markdown","metadata":{"id":"wPyFmXJpO4JW"},"source":["### Setting the threshold to just 2 occurrences puts the unk token as the most common one and if we change it to 1 we don't get any occurence."]},{"cell_type":"code","metadata":{"id":"30xDrlSUQFf5"},"source":[""],"execution_count":null,"outputs":[]}]}