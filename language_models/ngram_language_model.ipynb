{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ngram_language_model.ipynb","provenance":[],"mount_file_id":"1xZokeORwfxnvaeA_N8kgKZXQ9eBeGY0J","authorship_tag":"ABX9TyOfaByd0UnrlcXHVDWwfW+i"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4BNSa2BZykeA","executionInfo":{"status":"ok","timestamp":1607357188446,"user_tz":300,"elapsed":705,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"3246ad44-cb04-4e3b-9209-03d4f26775cd"},"source":["%cd /content/drive/MyDrive/Colab Notebooks/nlp/apps"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/nlp/apps\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RfQKAwany2KX","executionInfo":{"status":"ok","timestamp":1607357188805,"user_tz":300,"elapsed":1051,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["# This is just the first novel, we use it for testing proposes because it is smaller\n","testing_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/study in scarlet.txt'\n","\n","# This is the whole corpus\n","path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/sherlock_novels.txt'"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jGDtwcxnzAX1"},"source":["# Preprocessing the corpus\n","\n","These are the preprocessing steps that we are going to use:\n","\n","- lowercase the text\n","- remove special characters\n","- split text to list of sentences\n","- split sentences into list of words\n","\n","Notice that we will consider each line as a sentences for this language model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yGp6_0EwzcVk","executionInfo":{"status":"ok","timestamp":1607357190070,"user_tz":300,"elapsed":2308,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"f9b2bf60-9420-4326-affc-684aed741450"},"source":["import nltk\n","import re\n","\n","nltk.download('punkt')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"WtqGETKzzfiF","executionInfo":{"status":"ok","timestamp":1607357190072,"user_tz":300,"elapsed":2302,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def remove_special(sentence):\n","    \"\"\"\n","    Takes a sentence and only keeps .,?! and space\n","    as special characters.\n","    Args:\n","        sentence: str\n","    returns\n","        sentence: str. The full sentence cleaned of special characters\n","    \"\"\"\n","    sentence = re.sub(r'[^a-zA-Z0-9.,?! ]+', '', sentence)\n","\n","    return sentence\n","\n","def get_text(path):\n","    \"\"\"\n","    It reads a txt file and returns a string with all the corpus\n","    Args:\n","        path: str\n","    returns:\n","        text: str\n","    \"\"\"\n","    with open(path) as f:\n","        text = f.read()\n","\n","    return text\n","\n","def get_sentences(text):\n","    \"\"\"\n","    Takes a whole text removes special characters and divides it by \\n\n","    then it returns a list of list with the sentences\n","    Args:\n","        text: str\n","    returns:\n","        sentences: list\n","    \"\"\"\n","    text = text.lower()\n","    sentences = text.split('\\n')\n","    # also removes any empty line\n","    sentences = [remove_special(sentence.strip()) for sentence in sentences if len(sentence) > 0]\n","\n","    return sentences\n","\n","# Uncomment to see the 10 first sentences\n","\n","# text = get_text(testing_path)\n","# sentences = get_sentences(text)\n","# for s in sentences[:10]:\n","#     print(s)\n"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3k1oWSNSSYeN"},"source":["# Tokenize the corpus"]},{"cell_type":"code","metadata":{"id":"u612FuiD4KrQ","executionInfo":{"status":"ok","timestamp":1607357190075,"user_tz":300,"elapsed":2301,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def tokenize(sentences):\n","    \"\"\"\n","    It takes a list of strings that are the sentences\n","    and returns a list of list of tokens\n","    Args:\n","        sentences: list\n","    returns:\n","        tokenized_sentences: list\n","    \"\"\"\n","    tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n","\n","    return tokenized_sentences\n","\n","# Uncomment to test\n","\n","# text = get_text(testing_path)\n","# sentences = get_sentences(text)\n","# tokenized_sentences = tokenize(sentences[:10])\n","# for s in tokenized_sentences:\n","#     print(s)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4t-iQ_7e5oHd"},"source":["# Create training and test datsets"]},{"cell_type":"code","metadata":{"id":"j-CgKRo07Ex6","executionInfo":{"status":"ok","timestamp":1607357209242,"user_tz":300,"elapsed":21461,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["import random\n","\n","# First create a function to get the tokens\n","def get_tokens(path):\n","    \"\"\"\n","    It takes the path of a txt file and applies \n","    get_text(), get_sentences(), and tokenize()\n","    functions .\n","    Args:\n","        path: str\n","    returns:\n","        tokenized_sentences: list\n","    \"\"\"\n","    text = get_text(path)\n","    sentences = get_sentences(text)\n","    tokenized_sentences = tokenize(sentences)\n","\n","    return tokenized_sentences\n","\n","tokenized_sentences = get_tokens(path)\n","    "],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"raoNyKON8IZ3","executionInfo":{"status":"ok","timestamp":1607357209245,"user_tz":300,"elapsed":21459,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"1ead457b-910b-4428-df0e-e645f8d030b9"},"source":["random.seed(10)\n","random.shuffle(tokenized_sentences)\n","print(f'Amount of sentences {len(tokenized_sentences)}')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Amount of sentences 60198\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"wqGdvjl08idO"},"source":["### Because the corpus is big enough we can test using just 10% of the sentences"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lhNZjT2a9knc","executionInfo":{"status":"ok","timestamp":1607357519499,"user_tz":300,"elapsed":1203,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"5f0c3b02-df01-4bd6-ccb0-1c7ec3106f8b"},"source":["size = int(len(tokenized_sentences) * 0.9)\n","train = tokenized_sentences[:size]\n","test = tokenized_sentences[size:]\n","print(f'Training size: {len(train)}')\n","print(f'Testing size: {len(test)}')"],"execution_count":10,"outputs":[{"output_type":"stream","text":["Training size: 54178\n","Testing size: 6020\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k6SOLbYz-Ek8"},"source":[""],"execution_count":null,"outputs":[]}]}