{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"name_entity_recognition.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1-dRKJQlnholufpXyJ3rwtlMQiPRXEeoX","authorship_tag":"ABX9TyM9B4TgwilZxeeCjdiPsJMz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5JrleVZoY7t","executionInfo":{"status":"ok","timestamp":1607807724823,"user_tz":300,"elapsed":588,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"ddfe6461-24be-4058-bef0-6010e62f009a"},"source":["%cd /content/drive/MyDrive/Colab Notebooks/nlp/apps/name_entity_recognition"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/nlp/apps/name_entity_recognition\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FnDh_qHKoteC","executionInfo":{"status":"ok","timestamp":1607807725149,"user_tz":300,"elapsed":903,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/sherlock_novels.txt'\r\n","testing_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/study in scarlet.txt'\r\n","ner_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/ner_dataset.csv'\r\n","words_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/words.txt'\r\n","tags_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/tags.txt'\r\n","sentences_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/sentences.txt'\r\n","labels_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/labels.txt'\r\n","output_dir = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/name_entity_recognition/models/lstm_ner'"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"GzlX2lH4o2DT","executionInfo":{"status":"ok","timestamp":1607807727781,"user_tz":300,"elapsed":3524,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["!pip install -q -U trax"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lCpszR1QpHTs","executionInfo":{"status":"ok","timestamp":1607807740265,"user_tz":300,"elapsed":16002,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"4c008225-7a2a-4b60-9ec8-5f075870cb91"},"source":["import trax\r\n","import random\r\n","import pandas as pd\r\n","import numpy as np\r\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"iPV2vtOgwens"},"source":["# Get the data\r\n","\r\n","The columns of the dataset are:\r\n","\r\n","- The sentence number\r\n","- the word\r\n","- the part of speech of the word\r\n","- the tags\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"uiFPdXcnpP3q","executionInfo":{"status":"ok","timestamp":1607807740865,"user_tz":300,"elapsed":16589,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"c72d3c6e-6607-441d-fadc-553193ffa366"},"source":["df = pd.read_csv(ner_path, encoding='ISO-8859-1')\r\n","df.head()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>POS</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sentence: 1</td>\n","      <td>Thousands</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NaN</td>\n","      <td>of</td>\n","      <td>IN</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>NaN</td>\n","      <td>demonstrators</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>NaN</td>\n","      <td>have</td>\n","      <td>VBP</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NaN</td>\n","      <td>marched</td>\n","      <td>VBN</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Sentence #           Word  POS Tag\n","0  Sentence: 1      Thousands  NNS   O\n","1          NaN             of   IN   O\n","2          NaN  demonstrators  NNS   O\n","3          NaN           have  VBP   O\n","4          NaN        marched  VBN   O"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"zb_jWHT2SxAt"},"source":["# Understanding the tags\r\n","\r\n","The tag_map corresponds to one of the possible tags a word can have. The prepositions in the tags mean:\r\n","* I: Token is inside an entity.\r\n","* B: Token begins an entity."]},{"cell_type":"markdown","metadata":{"id":"KWeshCmNwUqM"},"source":["# Preprocess\r\n","\r\n","We are going to create two files: one with the unique tags and one with the unique words \r\n"]},{"cell_type":"code","metadata":{"id":"PrttC0DfyeC3","executionInfo":{"status":"ok","timestamp":1607807740882,"user_tz":300,"elapsed":16600,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def create_vocab(df, path='/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/'):\r\n","    \"\"\"\r\n","    Takes the ner dataset and creates a txt file\r\n","    with words that appear at least once.\r\n","    \"\"\"\r\n","    # Lowercase and upercase are treated as different words\r\n","    counts = df.Word.value_counts()\r\n","\r\n","    # add the unknown and padding tokens\r\n","    words = list(counts.index) + ['UNK', '<pad>']\r\n","    with open(path + 'words.txt', 'w') as f:\r\n","        for word in words:\r\n","            f.write(word + '\\n')\r\n","\r\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Phj62Ho0rcj","executionInfo":{"status":"ok","timestamp":1607807740884,"user_tz":300,"elapsed":16596,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def create_tags(df, path='/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/'):\r\n","    \"\"\"\r\n","    Takes the ner dataset and create a txt file with\r\n","    the different tags\r\n","    \"\"\"\r\n","    tags = list(df.Tag.value_counts().index)\r\n","    with open(path + 'tags.txt', 'w') as f:\r\n","        for tag in tags:\r\n","            if tag == 'O':\r\n","                continue\r\n","            f.write(tag + '\\n')\r\n","\r\n","        \r\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZuIsk-YdU8J0","executionInfo":{"status":"ok","timestamp":1607807740887,"user_tz":300,"elapsed":16594,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def create_sentences_labels(df, path='/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/'):\r\n","    \"\"\"\r\n","    Takes the ner dataset and extract the sentences\r\n","    and the labels to their respective txt file\r\n","    \"\"\"\r\n","    df_copy = df.copy()\r\n","    # Fill the na values with the sentence #\r\n","    df_copy.fillna(method='ffill', inplace=True)\r\n","\r\n","    # Store the unique sentences in a list\r\n","    sentences = list(df_copy['Sentence #'].unique())\r\n","    df_copy.set_index('Sentence #', drop=True, inplace=True)\r\n","\r\n","    # with the Sentence # column as index is easier to iterate\r\n","    # to get the individual sentences\r\n","    str_sentences = []\r\n","    labels = []\r\n","    print(f'Amount of sentences to process: {len(sentences)}')\r\n","    for i, sentence in enumerate(sentences):\r\n","        try:\r\n","            # get the individual words\r\n","            words = df_copy.loc[sentence].Word.values\r\n","\r\n","            # get the individual labels\r\n","            ind_labels = df_copy.loc[sentence].Tag.values\r\n","\r\n","            # Join the words and ind_labels in their\r\n","            # respective string\r\n","            str_sentence = ' '.join(words)\r\n","            label = ' '.join(ind_labels)\r\n","            str_sentences.append(str_sentence)\r\n","            labels.append(label)\r\n","\r\n","            if i % 500 == 0:\r\n","                print(f'{i} sentences processed')\r\n","        except Exception as e:\r\n","            print(e)\r\n","            print(f'Error in sentence {i}')\r\n","\r\n","\r\n","    # Save the str_sentences and labels into individual\r\n","    # txt files\r\n","    print('----Saving sentences----')\r\n","    with open(path + 'sentences.txt', 'w') as f:\r\n","        for sentence in str_sentences:\r\n","            f.write(sentence + '\\n')\r\n","    print('----Saving labels----')\r\n","    with open(path + 'labels.txt', 'w') as f:\r\n","        for label in labels:\r\n","            f.write(label + '\\n')\r\n","\r\n","\r\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"KwIXfwADOUyW","executionInfo":{"status":"ok","timestamp":1607807740888,"user_tz":300,"elapsed":16591,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def words_tags2index(words_path, tags_path):\r\n","    \"\"\"\r\n","    Takes the words.txt and tags.txt files and \r\n","    returns a dict mapping each word and token to\r\n","    a number\r\n","    \"\"\"\r\n","    word_map = dict()\r\n","    tag_map = dict()\r\n","    with open(words_path) as f:\r\n","        for i, word in enumerate(f.readlines(), 1):\r\n","            word_map[word.strip()] = i\r\n","    \r\n","    with open(tags_path) as f:\r\n","        for i, tag in enumerate(f.readlines(), 1):\r\n","            tag_map[tag.strip()] = i\r\n","    \r\n","    # set the O tag to 0\r\n","    tag_map['O'] = 0\r\n","    \r\n","    return word_map, tag_map\r\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lDE7ktES4PMR"},"source":["# Split the data into train, val, test\r\n"]},{"cell_type":"code","metadata":{"id":"rDPZ6a4a4PQL","executionInfo":{"status":"ok","timestamp":1607807740889,"user_tz":300,"elapsed":16587,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def split_sentences(sentences_path, labels_path, ratio=0.9):\r\n","    \"\"\"\r\n","    \"\"\"\r\n","    with open(sentences_path) as f:\r\n","        sentences = f.readlines()\r\n","\r\n","    with open(labels_path) as f:\r\n","        labels = f.readlines()\r\n","\r\n","    # 90% train, 5% val and 5% test\r\n","    sen_len = len(sentences)\r\n","    train_split = int(sen_len * ratio)\r\n","    x_train = sentences[:train_split]\r\n","    y_train = labels[:train_split]\r\n","\r\n","    val_split = sen_len - train_split\r\n","    val_split = int(val_split / 2)\r\n","    val_split = train_split + val_split\r\n","    \r\n","    x_val = sentences[train_split:val_split]\r\n","    y_val = labels[train_split:val_split]\r\n","\r\n","    x_test = sentences[val_split:]\r\n","    y_test = labels[val_split:]\r\n","\r\n","    return x_train, y_train, x_val, y_val, x_test, y_test\r\n","    \r\n","\r\n"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TU9hEIjMMn8q"},"source":["# Transform each sentence and labels to numbers\r\n"]},{"cell_type":"code","metadata":{"id":"d-6iKvdxMoFi","executionInfo":{"status":"ok","timestamp":1607807740890,"user_tz":300,"elapsed":16582,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def transform2numbers(word_map, tag_map, sentences, tags):\r\n","    \"\"\"\r\n","    \"\"\"\r\n","    data = []\r\n","    labels = []\r\n","    for sentence, tag in zip(sentences, tags):\r\n","        # replace each token by its index\r\n","        # if it is in the word_map else\r\n","        # use the UNK token\r\n","        tokens = [word_map[token] if token in word_map else word_map['UNK'] for token in sentence.strip().split(' ')]\r\n","        label = [tag_map[token] for token in tag.strip().split(' ')]\r\n","        data.append(tokens)\r\n","        labels.append(label)\r\n","\r\n","    return data, labels, len(data)\r\n","    \r\n"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QvaQJ46UVnJN"},"source":["# Data generator"]},{"cell_type":"code","metadata":{"id":"2SOXVZNzVnMu","executionInfo":{"status":"ok","timestamp":1607807740894,"user_tz":300,"elapsed":16576,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def data_generator(batch_size, x, y, pad, shuffle=False):\r\n","    # count the number of sentences\r\n","    num_sentences = len(x)\r\n","\r\n","    # create an array with the indexes of the sentences that can be shuffle\r\n","    sentences_index = [*range(num_sentences)]\r\n","    if shuffle:\r\n","        random.shuffle(sentences_index)\r\n","\r\n","    # track current location of x and y\r\n","    index = 0\r\n","    while True:\r\n","        # Temporal array to store the raw x data for this batch\r\n","        buffer_x = [0] * batch_size\r\n","        \r\n","        # Temporal array to store the raw y data for this batch\r\n","        buffer_y = [0] * batch_size\r\n","\r\n","        # create the batches\r\n","        max_len = 0\r\n","        for i in range(batch_size):\r\n","            if index >= num_sentences:\r\n","                # reset index to 0\r\n","                index = 0\r\n","                if shuffle:\r\n","                    random.shuffle(sentences_index)\r\n","\r\n","            buffer_x[i] = x[sentences_index[index]]\r\n","            buffer_y[i] = y[sentences_index[index]]\r\n","\r\n","            # lenght of current x\r\n","            lenx = len(buffer_x[i])\r\n","            if lenx > max_len:\r\n","                max_len = lenx\r\n","            \r\n","            index += 1\r\n","        # create X,Y, NumPy arrays of size (batch_size, max_len) 'full' of pad value\r\n","        X = np.full((batch_size, max_len), pad)\r\n","        Y = np.full((batch_size, max_len), pad)\r\n","        \r\n","        # copy values from lists to NumPy arrays. Use the buffered values\r\n","        for i in range(batch_size):\r\n","            # get the example (sentence as a tensor)\r\n","            # in buffer_x at the i index\r\n","            x_i = buffer_x[i]\r\n","\r\n","            # similarly, get the example's labels\r\n","            # in buffer_y at the i index\r\n","            y_i = buffer_y[i]\r\n","\r\n","            # Walk through each word in x_i\r\n","            for j in range(len(x_i)):\r\n","                # store the word in x_i at position j into X\r\n","                X[i, j] = x_i[j]\r\n","                \r\n","                # store the label in y_i at position j into Y\r\n","                Y[i, j] = y_i[j]\r\n","        \r\n","        yield((X,Y))\r\n","\r\n"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qSEjOOXSVnSf"},"source":["# Create model"]},{"cell_type":"code","metadata":{"id":"7qrvfucHVnVY","executionInfo":{"status":"ok","timestamp":1607807985435,"user_tz":300,"elapsed":708,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def create_model(tags=None, vocab_size=35181, emb_dim=50):\r\n","    model = trax.layers.Serial(\r\n","        trax.layers.Embedding(vocab_size, emb_dim),\r\n","        trax.layers.LSTM(emb_dim),\r\n","        trax.layers.Dense(len(tags)),\r\n","        trax.layers.LogSoftmax()\r\n","    )\r\n","\r\n","    return model"],"execution_count":20,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IRAVWZqnnNQv"},"source":["# Train model"]},{"cell_type":"code","metadata":{"id":"Gjn0aAjvnPnR","executionInfo":{"status":"ok","timestamp":1607807986274,"user_tz":300,"elapsed":751,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def create_data_streams(batch_size, train_data, train_labels, val_data, val_labels, word_map):\r\n","\r\n","    # create training data mask pad id=35180 for training.\r\n","    train_generator = trax.data.inputs.add_loss_weights(\r\n","        data_generator(batch_size, train_data, train_labels, word_map['<pad>'], shuffle=True),\r\n","        id_to_mask=word_map['<pad>']\r\n","    )\r\n","\r\n","    # create validation data\r\n","    val_generator = trax.data.inputs.add_loss_weights(\r\n","        data_generator(batch_size, val_data, val_labels, word_map['<pad>']),\r\n","        id_to_mask=word_map['pad']\r\n","    )\r\n","\r\n","    return train_generator, val_generator\r\n","\r\n","def train_model(model, train_generator, val_generator, n_steps, learning_rate=0.001, output_dir='/model'):\r\n","    print(f'This is the amount of steps needed to end traning: {n_steps}')\r\n","\r\n","    train_task = trax.supervised.training.TrainTask(\r\n","        train_generator,\r\n","        loss_layer=trax.layers.CrossEntropyLoss(),\r\n","        optimizer=trax.optimizers.Adam(learning_rate),\r\n","        n_steps_per_checkpoint=500\r\n","    )\r\n","\r\n","    val_task = trax.supervised.training.EvalTask(\r\n","        labeled_data=val_generator,\r\n","        metrics=[trax.layers.CrossEntropyLoss(), trax.layers.Accuracy(),],\r\n","        n_eval_batches=10\r\n","    )\r\n","\r\n","    training_loop = trax.supervised.training.Loop(\r\n","        model, \r\n","        train_task,\r\n","        eval_tasks=[val_task],\r\n","        output_dir=output_dir\r\n","    )\r\n","\r\n","    training_loop.run(n_steps)\r\n","\r\n","    return training_loop"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"mINqyoEPPhjs","executionInfo":{"status":"ok","timestamp":1607807987398,"user_tz":300,"elapsed":1520,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["#create_vocab(df)\n","#create_tags(df)\n","#create_sentences_labels(df)\n","\n","# Dicts word to index and tag to index\n","word_map, tag_map = words_tags2index(words_path, tags_path)\n","\n","# with the sentences and labels created split the data\n","x_train, y_train, x_val, y_val, x_test, y_test = split_sentences(sentences_path, labels_path)\n","\n","# Transform the splits into numbers\n","train_data, train_labels, train_size = transform2numbers(word_map, tag_map, x_train, y_train)\n","val_data, val_labels, val_size = transform2numbers(word_map, tag_map, x_val, y_val)\n","test_data, test_labels, test_size = transform2numbers(word_map, tag_map, x_test, y_test)\n","\n","\n"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"RHu-ccR2nPs9"},"source":["batch_size = 64\r\n","emb_dim = 50\r\n","learning_rate = 0.01\r\n","epochs = 10\r\n","n_steps = int(len(train_data) / batch_size) * epochs\r\n","\r\n","model = create_model(tags=tag_map)\r\n","train_generator, val_generator = create_data_streams(batch_size, train_data, train_labels, val_data, val_labels, word_map)\r\n","training_loop = train_model(model, train_generator, val_generator, learning_rate=learning_rate, n_steps=n_steps, output_dir=output_dir)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-Bb94JbBzYCX"},"source":["# Computing accuracy on the evaluation data\r\n","\r\n","Let's see what our model needs as output, for this we need to know that our model prediction has 3 axes: \r\n","\r\n","- the number of examples\r\n","- the number of words in each example (padded to be as long as the longest sentence in the batch)\r\n","- the number of possible targets (the 17 named entity tags)."]},{"cell_type":"code","metadata":{"id":"b75AHdVx6g1A","executionInfo":{"status":"ok","timestamp":1607809957859,"user_tz":300,"elapsed":713,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def evaluate_prediction(predictions, labels, pad):\r\n","    outputs = np.argmax(predictions, axis=2)\r\n","    print(f'outputs shape: {outputs.shape}')\r\n","\r\n","    mask = labels != pad\r\n","    print(f'mask shape: {mask.shape} mask[0][20:30] {mask[0][20:30]}')\r\n","\r\n","    accuracy = np.sum(outputs == labels) / np.float(np.sum(mask))\r\n","\r\n","    return accuracy"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aMlCdxBq7pIP","executionInfo":{"status":"ok","timestamp":1607809959299,"user_tz":300,"elapsed":666,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"ad674c2f-9abe-41df-b034-f68de27c9a0a"},"source":["# create the evaluation inputs\r\n","x, y = next(data_generator(len(test_data), test_data, test_labels, word_map['<pad>']))\r\n","print(\"input shapes\", x.shape, y.shape)"],"execution_count":30,"outputs":[{"output_type":"stream","text":["input shapes (2398, 70) (2398, 70)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":338},"id":"pVMYTc7J8CoG","executionInfo":{"status":"error","timestamp":1607809960768,"user_tz":300,"elapsed":1013,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"0fe2a9a8-2535-4c15-cf7b-ed89acc3431e"},"source":["accuracy = evaluate_prediction(model(x), y, word_map['<pad>'])\r\n","print(\"accuracy: \", accuracy)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["outputs shape: (2398, 70)\n"],"name":"stdout"},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-31-24866db82b60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'<pad>'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-29-4aaf689fbaa7>\u001b[0m in \u001b[0;36mevaluate_prediction\u001b[0;34m(predictions, labels, pad)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'mask shape: {mask.shape} mask[0][20:30] {masl[0][20:30]}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'masl' is not defined"]}]},{"cell_type":"code","metadata":{"id":"zGiqKxVj8I8F"},"source":[""],"execution_count":null,"outputs":[]}]}