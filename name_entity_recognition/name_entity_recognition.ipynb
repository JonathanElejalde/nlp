{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"name_entity_recognition.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1-dRKJQlnholufpXyJ3rwtlMQiPRXEeoX","authorship_tag":"ABX9TyMem4qP2gDF5gexWdmiOTWx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5JrleVZoY7t","executionInfo":{"status":"ok","timestamp":1607743718105,"user_tz":300,"elapsed":939,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"05861bd6-1322-4129-9108-bd61570c831b"},"source":["%cd /content/drive/MyDrive/Colab Notebooks/nlp/apps/name_entity_recognition"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/nlp/apps/name_entity_recognition\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FnDh_qHKoteC","executionInfo":{"status":"ok","timestamp":1607743718107,"user_tz":300,"elapsed":919,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/sherlock_novels.txt'\r\n","testing_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/study in scarlet.txt'\r\n","ner_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/ner_dataset.csv'\r\n","words_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/words.txt'\r\n","tags_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/tags.txt'\r\n","sentences_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/sentences.txt'\r\n","labels_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/labels.txt'\r\n","output_dir = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/name_entity_recognition/models/lstm_ner'"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GzlX2lH4o2DT","executionInfo":{"status":"ok","timestamp":1607743728461,"user_tz":300,"elapsed":11263,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"fc0bfa6b-1aea-48c7-f5d1-6b4cce5b95ee"},"source":["!pip install -q -U trax"],"execution_count":3,"outputs":[{"output_type":"stream","text":["\u001b[K     |████████████████████████████████| 471kB 13.7MB/s \n","\u001b[K     |████████████████████████████████| 174kB 58.6MB/s \n","\u001b[K     |████████████████████████████████| 2.6MB 50.4MB/s \n","\u001b[K     |████████████████████████████████| 71kB 12.2MB/s \n","\u001b[K     |████████████████████████████████| 348kB 22.3MB/s \n","\u001b[K     |████████████████████████████████| 1.1MB 60.1MB/s \n","\u001b[K     |████████████████████████████████| 3.7MB 49.1MB/s \n","\u001b[K     |████████████████████████████████| 1.4MB 50.4MB/s \n","\u001b[K     |████████████████████████████████| 2.9MB 64.5MB/s \n","\u001b[K     |████████████████████████████████| 890kB 48.9MB/s \n","\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lCpszR1QpHTs","executionInfo":{"status":"ok","timestamp":1607743745407,"user_tz":300,"elapsed":28200,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"abd9d931-0088-428d-8a4e-b7d9b13e3051"},"source":["import trax\r\n","import random\r\n","import pandas as pd\r\n","import numpy as np\r\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n","WARNING:root:Argument blacklist is deprecated. Please use denylist.\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"iPV2vtOgwens"},"source":["# Get the data\r\n","\r\n","The columns of the dataset are:\r\n","\r\n","- The sentence number\r\n","- the word\r\n","- the part of speech of the word\r\n","- the tags\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"uiFPdXcnpP3q","executionInfo":{"status":"ok","timestamp":1607743747867,"user_tz":300,"elapsed":30631,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"06c15799-99de-49ca-b097-8ba8916cf6c6"},"source":["df = pd.read_csv(ner_path, encoding='ISO-8859-1')\r\n","df.head()"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>POS</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sentence: 1</td>\n","      <td>Thousands</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NaN</td>\n","      <td>of</td>\n","      <td>IN</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>NaN</td>\n","      <td>demonstrators</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>NaN</td>\n","      <td>have</td>\n","      <td>VBP</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NaN</td>\n","      <td>marched</td>\n","      <td>VBN</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Sentence #           Word  POS Tag\n","0  Sentence: 1      Thousands  NNS   O\n","1          NaN             of   IN   O\n","2          NaN  demonstrators  NNS   O\n","3          NaN           have  VBP   O\n","4          NaN        marched  VBN   O"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"zb_jWHT2SxAt"},"source":["# Understanding the tags\r\n","\r\n","The tag_map corresponds to one of the possible tags a word can have. The prepositions in the tags mean:\r\n","* I: Token is inside an entity.\r\n","* B: Token begins an entity."]},{"cell_type":"markdown","metadata":{"id":"KWeshCmNwUqM"},"source":["# Preprocess\r\n","\r\n","We are going to create two files: one with the unique tags and one with the unique words \r\n"]},{"cell_type":"code","metadata":{"id":"PrttC0DfyeC3","executionInfo":{"status":"ok","timestamp":1607743747872,"user_tz":300,"elapsed":30630,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def create_vocab(df, path='/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/'):\r\n","    \"\"\"\r\n","    Takes the ner dataset and creates a txt file\r\n","    with words that appear at least once.\r\n","    \"\"\"\r\n","    # Lowercase and upercase are treated as different words\r\n","    counts = df.Word.value_counts()\r\n","\r\n","    # add the unknown and padding tokens\r\n","    words = list(counts.index) + ['UNK', '<pad>']\r\n","    with open(path + 'words.txt', 'w') as f:\r\n","        for word in words:\r\n","            f.write(word + '\\n')\r\n","\r\n"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Phj62Ho0rcj","executionInfo":{"status":"ok","timestamp":1607743747875,"user_tz":300,"elapsed":30627,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def create_tags(df, path='/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/'):\r\n","    \"\"\"\r\n","    Takes the ner dataset and create a txt file with\r\n","    the different tags\r\n","    \"\"\"\r\n","    tags = list(df.Tag.value_counts().index)\r\n","    with open(path + 'tags.txt', 'w') as f:\r\n","        for tag in tags:\r\n","            if tag == 'O':\r\n","                continue\r\n","            f.write(tag + '\\n')\r\n","\r\n","        \r\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZuIsk-YdU8J0","executionInfo":{"status":"ok","timestamp":1607743747876,"user_tz":300,"elapsed":30623,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def create_sentences_labels(df, path='/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/'):\r\n","    \"\"\"\r\n","    Takes the ner dataset and extract the sentences\r\n","    and the labels to their respective txt file\r\n","    \"\"\"\r\n","    df_copy = df.copy()\r\n","    # Fill the na values with the sentence #\r\n","    df_copy.fillna(method='ffill', inplace=True)\r\n","\r\n","    # Store the unique sentences in a list\r\n","    sentences = list(df_copy['Sentence #'].unique())\r\n","    df_copy.set_index('Sentence #', drop=True, inplace=True)\r\n","\r\n","    # with the Sentence # column as index is easier to iterate\r\n","    # to get the individual sentences\r\n","    str_sentences = []\r\n","    labels = []\r\n","    print(f'Amount of sentences to process: {len(sentences)}')\r\n","    for i, sentence in enumerate(sentences):\r\n","        try:\r\n","            # get the individual words\r\n","            words = df_copy.loc[sentence].Word.values\r\n","\r\n","            # get the individual labels\r\n","            ind_labels = df_copy.loc[sentence].Tag.values\r\n","\r\n","            # Join the words and ind_labels in their\r\n","            # respective string\r\n","            str_sentence = ' '.join(words)\r\n","            label = ' '.join(ind_labels)\r\n","            str_sentences.append(str_sentence)\r\n","            labels.append(label)\r\n","\r\n","            if i % 500 == 0:\r\n","                print(f'{i} sentences processed')\r\n","        except Exception as e:\r\n","            print(e)\r\n","            print(f'Error in sentence {i}')\r\n","\r\n","\r\n","    # Save the str_sentences and labels into individual\r\n","    # txt files\r\n","    print('----Saving sentences----')\r\n","    with open(path + 'sentences.txt', 'w') as f:\r\n","        for sentence in str_sentences:\r\n","            f.write(sentence + '\\n')\r\n","    print('----Saving labels----')\r\n","    with open(path + 'labels.txt', 'w') as f:\r\n","        for label in labels:\r\n","            f.write(label + '\\n')\r\n","\r\n","\r\n"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"KwIXfwADOUyW","executionInfo":{"status":"ok","timestamp":1607743747878,"user_tz":300,"elapsed":30619,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def words_tags2index(words_path, tags_path):\r\n","    \"\"\"\r\n","    Takes the words.txt and tags.txt files and \r\n","    returns a dict mapping each word and token to\r\n","    a number\r\n","    \"\"\"\r\n","    word_map = dict()\r\n","    tag_map = dict()\r\n","    with open(words_path) as f:\r\n","        for i, word in enumerate(f.readlines(), 1):\r\n","            word_map[word.strip()] = i\r\n","    \r\n","    with open(tags_path) as f:\r\n","        for i, tag in enumerate(f.readlines(), 1):\r\n","            tag_map[tag.strip()] = i\r\n","    \r\n","    # set the O tag to 0\r\n","    tag_map['O'] = 0\r\n","    \r\n","    return word_map, tag_map\r\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lDE7ktES4PMR"},"source":["# Split the data into train, val, test\r\n"]},{"cell_type":"code","metadata":{"id":"rDPZ6a4a4PQL","executionInfo":{"status":"ok","timestamp":1607743747879,"user_tz":300,"elapsed":30614,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def split_sentences(sentences_path, labels_path, ratio=0.9):\r\n","    \"\"\"\r\n","    \"\"\"\r\n","    with open(sentences_path) as f:\r\n","        sentences = f.readlines()\r\n","\r\n","    with open(labels_path) as f:\r\n","        labels = f.readlines()\r\n","\r\n","    # 90% train, 5% val and 5% test\r\n","    sen_len = len(sentences)\r\n","    train_split = int(sen_len * ratio)\r\n","    x_train = sentences[:train_split]\r\n","    y_train = labels[:train_split]\r\n","\r\n","    val_split = sen_len - train_split\r\n","    val_split = int(val_split / 2)\r\n","    val_split = train_split + val_split\r\n","    \r\n","    x_val = sentences[train_split:val_split]\r\n","    y_val = labels[train_split:val_split]\r\n","\r\n","    x_test = sentences[val_split:]\r\n","    y_test = labels[val_split:]\r\n","\r\n","    return x_train, y_train, x_val, y_val, x_test, y_test\r\n","    \r\n","\r\n"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TU9hEIjMMn8q"},"source":["# Transform each sentence and labels to numbers\r\n"]},{"cell_type":"code","metadata":{"id":"d-6iKvdxMoFi","executionInfo":{"status":"ok","timestamp":1607743747880,"user_tz":300,"elapsed":30608,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def transform2numbers(word_map, tag_map, sentences, tags):\r\n","    \"\"\"\r\n","    \"\"\"\r\n","    data = []\r\n","    labels = []\r\n","    for sentence, tag in zip(sentences, tags):\r\n","        # replace each token by its index\r\n","        # if it is in the word_map else\r\n","        # use the UNK token\r\n","        tokens = [word_map[token] if token in word_map else word_map['UNK'] for token in sentence.strip().split(' ')]\r\n","        label = [tag_map[token] for token in tag.strip().split(' ')]\r\n","        data.append(tokens)\r\n","        labels.append(label)\r\n","\r\n","    return data, labels, len(data)\r\n","    \r\n"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QvaQJ46UVnJN"},"source":["# Data generator"]},{"cell_type":"code","metadata":{"id":"2SOXVZNzVnMu","executionInfo":{"status":"ok","timestamp":1607743747881,"user_tz":300,"elapsed":30603,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def data_generator(batch_size, x, y, pad, shuffle=False):\r\n","    # count the number of sentences\r\n","    num_sentences = len(x)\r\n","\r\n","    # create an array with the indexes of the sentences that can be shuffle\r\n","    sentences_index = [*range(num_sentences)]\r\n","    if shuffle:\r\n","        random.shuffle(sentences_index)\r\n","\r\n","    # track current location of x and y\r\n","    index = 0\r\n","    while True:\r\n","        # Temporal array to store the raw x data for this batch\r\n","        buffer_x = [0] * batch_size\r\n","        \r\n","        # Temporal array to store the raw y data for this batch\r\n","        buffer_y = [0] * batch_size\r\n","\r\n","        # create the batches\r\n","        max_len = 0\r\n","        for i in range(batch_size):\r\n","            if index >= num_sentences:\r\n","                # reset index to 0\r\n","                index = 0\r\n","                if shuffle:\r\n","                    random.shuffle(sentences_index)\r\n","\r\n","            buffer_x[i] = x[sentences_index[index]]\r\n","            buffer_y[i] = y[sentences_index[index]]\r\n","\r\n","            # lenght of current x\r\n","            lenx = len(buffer_x[i])\r\n","            if lenx > max_len:\r\n","                max_len = lenx\r\n","            \r\n","            index += 1\r\n","        # create X,Y, NumPy arrays of size (batch_size, max_len) 'full' of pad value\r\n","        X = np.full((batch_size, max_len), pad)\r\n","        Y = np.full((batch_size, max_len), pad)\r\n","        \r\n","        # copy values from lists to NumPy arrays. Use the buffered values\r\n","        for i in range(batch_size):\r\n","            # get the example (sentence as a tensor)\r\n","            # in buffer_x at the i index\r\n","            x_i = buffer_x[i]\r\n","\r\n","            # similarly, get the example's labels\r\n","            # in buffer_y at the i index\r\n","            y_i = buffer_y[i]\r\n","\r\n","            # Walk through each word in x_i\r\n","            for j in range(len(x_i)):\r\n","                # store the word in x_i at position j into X\r\n","                X[i, j] = x_i[j]\r\n","                \r\n","                # store the label in y_i at position j into Y\r\n","                Y[i, j] = y_i[j]\r\n","        \r\n","        yield((X,Y))\r\n","\r\n"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qSEjOOXSVnSf"},"source":["# Create model"]},{"cell_type":"code","metadata":{"id":"7qrvfucHVnVY","executionInfo":{"status":"ok","timestamp":1607743747883,"user_tz":300,"elapsed":30599,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def create_model(tags=None, vocab_size=35181, emb_dim=100):\r\n","    model = trax.layers.Serial(\r\n","        trax.layers.Embedding(vocab_size, emb_dim),\r\n","        trax.layers.LSTM(emb_dim),\r\n","        trax.layers.Dense(len(tags)),\r\n","        trax.layers.LogSoftmax()\r\n","    )\r\n","\r\n","    return model"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IRAVWZqnnNQv"},"source":["# Train model"]},{"cell_type":"code","metadata":{"id":"Gjn0aAjvnPnR","executionInfo":{"status":"ok","timestamp":1607743791266,"user_tz":300,"elapsed":637,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def create_data_streams(batch_size, train_data, train_labels, val_data, val_labels, word_map):\r\n","\r\n","    # create training data mask pad id=35180 for training.\r\n","    train_generator = trax.data.inputs.add_loss_weights(\r\n","        data_generator(batch_size, train_data, train_labels, word_map['<pad>'], shuffle=True),\r\n","        id_to_mask=word_map['<pad>']\r\n","    )\r\n","\r\n","    # create validation data\r\n","    val_generator = trax.data.inputs.add_loss_weights(\r\n","        data_generator(batch_size, val_data, val_labels, word_map['<pad>']),\r\n","        id_to_mask=word_map['pad']\r\n","    )\r\n","\r\n","    return train_generator, val_generator\r\n","\r\n","def train_model(model, train_generator, val_generator, n_steps, learning_rate=0.001, output_dir='/model'):\r\n","    print(f'This is the amount of steps needed to end traning: {n_steps}')\r\n","\r\n","    train_task = trax.supervised.training.TrainTask(\r\n","        train_generator,\r\n","        loss_layer=trax.layers.CrossEntropyLoss(),\r\n","        optimizer=trax.optimizers.Adam(learning_rate),\r\n","        n_steps_per_checkpoint=500\r\n","    )\r\n","\r\n","    val_task = trax.supervised.training.EvalTask(\r\n","        labeled_data=val_generator,\r\n","        metrics=[trax.layers.CrossEntropyLoss(), trax.layers.Accuracy(),],\r\n","        n_eval_batches=10\r\n","    )\r\n","\r\n","    training_loop = trax.supervised.training.Loop(\r\n","        model, \r\n","        train_task,\r\n","        eval_tasks=[val_task],\r\n","        output_dir=output_dir\r\n","    )\r\n","\r\n","    training_loop.run(n_steps)\r\n","\r\n","    return training_loop"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"mINqyoEPPhjs","executionInfo":{"status":"ok","timestamp":1607743792859,"user_tz":300,"elapsed":1093,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["#create_vocab(df)\n","#create_tags(df)\n","#create_sentences_labels(df)\n","\n","# Dicts word to index and tag to index\n","word_map, tag_map = words_tags2index(words_path, tags_path)\n","\n","# with the sentences and labels created split the data\n","x_train, y_train, x_val, y_val, x_test, y_test = split_sentences(sentences_path, labels_path)\n","\n","# Transform the splits into numbers\n","train_data, train_labels, train_size = transform2numbers(word_map, tag_map, x_train, y_train)\n","val_data, val_labels, val_size = transform2numbers(word_map, tag_map, x_val, y_val)\n","test_data, test_labels, test_size = transform2numbers(word_map, tag_map, x_test, y_test)\n","\n","\n"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RHu-ccR2nPs9","outputId":"3ed82da7-e5d7-452e-920f-d600a72a3a8a"},"source":["batch_size = 64\r\n","emb_dim = 50\r\n","epochs = 20\r\n","n_steps = int(len(train_data) / batch_size) * epochs\r\n","\r\n","model = create_model(tags=tag_map)\r\n","train_generator, val_generator = create_data_streams(batch_size, train_data, train_labels, val_data, val_labels, word_map)\r\n","training_loop = train_model(model, train_generator, val_generator, n_steps=n_steps)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["This is the amount of steps needed to end traning: 13480\n","\n","Step   1000: Ran 500 train steps in 83.79 secs\n","Step   1000: train CrossEntropyLoss |  0.13515127\n","Step   1000: eval  CrossEntropyLoss |  0.06577097\n","Step   1000: eval          Accuracy |  0.49716882\n","\n","Step   1500: Ran 500 train steps in 9.44 secs\n","Step   1500: train CrossEntropyLoss |  0.10438364\n","Step   1500: eval  CrossEntropyLoss |  0.06128471\n","Step   1500: eval          Accuracy |  0.46283979\n","\n","Step   2000: Ran 500 train steps in 8.14 secs\n","Step   2000: train CrossEntropyLoss |  0.09457259\n","Step   2000: eval  CrossEntropyLoss |  0.05736920\n","Step   2000: eval          Accuracy |  0.51485153\n","\n","Step   2500: Ran 500 train steps in 8.15 secs\n","Step   2500: train CrossEntropyLoss |  0.08399539\n","Step   2500: eval  CrossEntropyLoss |  0.06878502\n","Step   2500: eval          Accuracy |  0.51982425\n","\n","Step   3000: Ran 500 train steps in 8.17 secs\n","Step   3000: train CrossEntropyLoss |  0.07628500\n","Step   3000: eval  CrossEntropyLoss |  0.06083496\n","Step   3000: eval          Accuracy |  0.48723035\n","\n","Step   3500: Ran 500 train steps in 8.23 secs\n","Step   3500: train CrossEntropyLoss |  0.07252492\n","Step   3500: eval  CrossEntropyLoss |  0.06614752\n","Step   3500: eval          Accuracy |  0.47259284\n","\n","Step   4000: Ran 500 train steps in 8.19 secs\n","Step   4000: train CrossEntropyLoss |  0.06872863\n","Step   4000: eval  CrossEntropyLoss |  0.05675900\n","Step   4000: eval          Accuracy |  0.50723538\n","\n","Step   4500: Ran 500 train steps in 8.19 secs\n","Step   4500: train CrossEntropyLoss |  0.06641380\n","Step   4500: eval  CrossEntropyLoss |  0.07254150\n","Step   4500: eval          Accuracy |  0.51530419\n","\n","Step   5000: Ran 500 train steps in 8.08 secs\n","Step   5000: train CrossEntropyLoss |  0.06024882\n","Step   5000: eval  CrossEntropyLoss |  0.06975159\n","Step   5000: eval          Accuracy |  0.48573891\n","\n","Step   5500: Ran 500 train steps in 8.17 secs\n","Step   5500: train CrossEntropyLoss |  0.05849721\n","Step   5500: eval  CrossEntropyLoss |  0.07504268\n","Step   5500: eval          Accuracy |  0.48250719\n","\n","Step   6000: Ran 500 train steps in 8.20 secs\n","Step   6000: train CrossEntropyLoss |  0.05720675\n","Step   6000: eval  CrossEntropyLoss |  0.06721582\n","Step   6000: eval          Accuracy |  0.51209369\n","\n","Step   6500: Ran 500 train steps in 8.29 secs\n","Step   6500: train CrossEntropyLoss |  0.05284258\n","Step   6500: eval  CrossEntropyLoss |  0.07545842\n","Step   6500: eval          Accuracy |  0.50655960\n","\n","Step   7000: Ran 500 train steps in 8.36 secs\n","Step   7000: train CrossEntropyLoss |  0.04959002\n","Step   7000: eval  CrossEntropyLoss |  0.08361462\n","Step   7000: eval          Accuracy |  0.47988098\n","\n","Step   7500: Ran 500 train steps in 8.45 secs\n","Step   7500: train CrossEntropyLoss |  0.04775763\n","Step   7500: eval  CrossEntropyLoss |  0.07186819\n","Step   7500: eval          Accuracy |  0.49274147\n","\n","Step   8000: Ran 500 train steps in 8.31 secs\n","Step   8000: train CrossEntropyLoss |  0.04633858\n","Step   8000: eval  CrossEntropyLoss |  0.08795484\n","Step   8000: eval          Accuracy |  0.50512450\n","\n","Step   8500: Ran 500 train steps in 8.40 secs\n","Step   8500: train CrossEntropyLoss |  0.04301491\n","Step   8500: eval  CrossEntropyLoss |  0.08643040\n","Step   8500: eval          Accuracy |  0.49297668\n","\n","Step   9000: Ran 500 train steps in 8.25 secs\n","Step   9000: train CrossEntropyLoss |  0.04011077\n","Step   9000: eval  CrossEntropyLoss |  0.08460845\n","Step   9000: eval          Accuracy |  0.46196026\n","\n","Step   9500: Ran 500 train steps in 8.25 secs\n","Step   9500: train CrossEntropyLoss |  0.03875836\n","Step   9500: eval  CrossEntropyLoss |  0.08666007\n","Step   9500: eval          Accuracy |  0.51153384\n","\n","Step  10000: Ran 500 train steps in 8.41 secs\n","Step  10000: train CrossEntropyLoss |  0.03771851\n","Step  10000: eval  CrossEntropyLoss |  0.11373096\n","Step  10000: eval          Accuracy |  0.52191188\n","\n","Step  10500: Ran 500 train steps in 8.06 secs\n","Step  10500: train CrossEntropyLoss |  0.03445547\n","Step  10500: eval  CrossEntropyLoss |  0.08975812\n","Step  10500: eval          Accuracy |  0.48191256\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LnC28Ce4xP9G","executionInfo":{"status":"aborted","timestamp":1607743750943,"user_tz":300,"elapsed":33640,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":[""],"execution_count":null,"outputs":[]}]}