{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"name_entity_recognition.ipynb","provenance":[],"mount_file_id":"1-dRKJQlnholufpXyJ3rwtlMQiPRXEeoX","authorship_tag":"ABX9TyPnMPoK2lR7gI2b6HVkUFRj"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5JrleVZoY7t","executionInfo":{"status":"ok","timestamp":1607732465292,"user_tz":300,"elapsed":688,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"e14dc822-c222-4ed7-d1d7-23376a606112"},"source":["%cd /content/drive/MyDrive/Colab Notebooks/nlp/apps/name_entity_recognition"],"execution_count":61,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/nlp/apps/name_entity_recognition\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"FnDh_qHKoteC","executionInfo":{"status":"ok","timestamp":1607732465294,"user_tz":300,"elapsed":679,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/sherlock_novels.txt'\r\n","testing_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/study in scarlet.txt'\r\n","ner_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/ner_dataset.csv'\r\n","words_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/words.txt'\r\n","tags_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/tags.txt'\r\n","sentences_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/sentences.txt'\r\n","labels_path = '/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/labels.txt'"],"execution_count":62,"outputs":[]},{"cell_type":"code","metadata":{"id":"GzlX2lH4o2DT","executionInfo":{"status":"ok","timestamp":1607732468659,"user_tz":300,"elapsed":4039,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["!pip install -q -U trax"],"execution_count":63,"outputs":[]},{"cell_type":"code","metadata":{"id":"lCpszR1QpHTs","executionInfo":{"status":"ok","timestamp":1607732468664,"user_tz":300,"elapsed":4034,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["import trax\r\n","import random\r\n","import pandas as pd\r\n","import numpy as np\r\n"],"execution_count":64,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iPV2vtOgwens"},"source":["# Get the data\r\n","\r\n","The columns of the dataset are:\r\n","\r\n","- The sentence number\r\n","- the word\r\n","- the part of speech of the word\r\n","- the tags\r\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"uiFPdXcnpP3q","executionInfo":{"status":"ok","timestamp":1607732469372,"user_tz":300,"elapsed":4513,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}},"outputId":"54a74bc6-ef41-483e-d297-9e9ba2daacaa"},"source":["df = pd.read_csv(ner_path, encoding='ISO-8859-1')\r\n","df.head()"],"execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>POS</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sentence: 1</td>\n","      <td>Thousands</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>NaN</td>\n","      <td>of</td>\n","      <td>IN</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>NaN</td>\n","      <td>demonstrators</td>\n","      <td>NNS</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>NaN</td>\n","      <td>have</td>\n","      <td>VBP</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>NaN</td>\n","      <td>marched</td>\n","      <td>VBN</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Sentence #           Word  POS Tag\n","0  Sentence: 1      Thousands  NNS   O\n","1          NaN             of   IN   O\n","2          NaN  demonstrators  NNS   O\n","3          NaN           have  VBP   O\n","4          NaN        marched  VBN   O"]},"metadata":{"tags":[]},"execution_count":65}]},{"cell_type":"markdown","metadata":{"id":"zb_jWHT2SxAt"},"source":["# Understanding the tags\r\n","\r\n","The tag_map corresponds to one of the possible tags a word can have. The prepositions in the tags mean:\r\n","* I: Token is inside an entity.\r\n","* B: Token begins an entity."]},{"cell_type":"markdown","metadata":{"id":"KWeshCmNwUqM"},"source":["# Preprocess\r\n","\r\n","We are going to create two files: one with the unique tags and one with the unique words \r\n"]},{"cell_type":"code","metadata":{"id":"PrttC0DfyeC3","executionInfo":{"status":"ok","timestamp":1607732469375,"user_tz":300,"elapsed":4505,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def create_vocab(df, path='/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/'):\r\n","    \"\"\"\r\n","    Takes the ner dataset and creates a txt file\r\n","    with words that appear at least once.\r\n","    \"\"\"\r\n","    # Lowercase and upercase are treated as different words\r\n","    counts = df.Word.value_counts()\r\n","\r\n","    # add the unknown and padding tokens\r\n","    words = list(counts.index) + ['UNK', '<pad>']\r\n","    with open(path + 'words.txt', 'w') as f:\r\n","        for word in words:\r\n","            f.write(word + '\\n')\r\n","\r\n"],"execution_count":66,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Phj62Ho0rcj","executionInfo":{"status":"ok","timestamp":1607732469376,"user_tz":300,"elapsed":4501,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def create_tags(df, path='/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/'):\r\n","    \"\"\"\r\n","    Takes the ner dataset and create a txt file with\r\n","    the different tags\r\n","    \"\"\"\r\n","    tags = list(df.Tag.value_counts().index)\r\n","    with open(path + 'tags.txt', 'w') as f:\r\n","        for tag in tags:\r\n","            if tag == 'O':\r\n","                continue\r\n","            f.write(tag + '\\n')\r\n","\r\n","        \r\n"],"execution_count":67,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZuIsk-YdU8J0","executionInfo":{"status":"ok","timestamp":1607732469378,"user_tz":300,"elapsed":4499,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def create_sentences_labels(df, path='/content/drive/MyDrive/Colab Notebooks/nlp/apps/data/'):\r\n","    \"\"\"\r\n","    Takes the ner dataset and extract the sentences\r\n","    and the labels to their respective txt file\r\n","    \"\"\"\r\n","    df_copy = df.copy()\r\n","    # Fill the na values with the sentence #\r\n","    df_copy.fillna(method='ffill', inplace=True)\r\n","\r\n","    # Store the unique sentences in a list\r\n","    sentences = list(df_copy['Sentence #'].unique())\r\n","    df_copy.set_index('Sentence #', drop=True, inplace=True)\r\n","\r\n","    # with the Sentence # column as index is easier to iterate\r\n","    # to get the individual sentences\r\n","    str_sentences = []\r\n","    labels = []\r\n","    print(f'Amount of sentences to process: {len(sentences)}')\r\n","    for i, sentence in enumerate(sentences):\r\n","        try:\r\n","            # get the individual words\r\n","            words = df_copy.loc[sentence].Word.values\r\n","\r\n","            # get the individual labels\r\n","            ind_labels = df_copy.loc[sentence].Tag.values\r\n","\r\n","            # Join the words and ind_labels in their\r\n","            # respective string\r\n","            str_sentence = ' '.join(words)\r\n","            label = ' '.join(ind_labels)\r\n","            str_sentences.append(str_sentence)\r\n","            labels.append(label)\r\n","\r\n","            if i % 500 == 0:\r\n","                print(f'{i} sentences processed')\r\n","        except Exception as e:\r\n","            print(e)\r\n","            print(f'Error in sentence {i}')\r\n","\r\n","\r\n","    # Save the str_sentences and labels into individual\r\n","    # txt files\r\n","    print('----Saving sentences----')\r\n","    with open(path + 'sentences.txt', 'w') as f:\r\n","        for sentence in str_sentences:\r\n","            f.write(sentence + '\\n')\r\n","    print('----Saving labels----')\r\n","    with open(path + 'labels.txt', 'w') as f:\r\n","        for label in labels:\r\n","            f.write(label + '\\n')\r\n","\r\n","\r\n"],"execution_count":68,"outputs":[]},{"cell_type":"code","metadata":{"id":"KwIXfwADOUyW","executionInfo":{"status":"ok","timestamp":1607732469379,"user_tz":300,"elapsed":4496,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def words_tags2index(words_path, tags_path):\r\n","    \"\"\"\r\n","    Takes the words.txt and tags.txt files and \r\n","    returns a dict mapping each word and token to\r\n","    a number\r\n","    \"\"\"\r\n","    word_map = dict()\r\n","    tag_map = dict()\r\n","    with open(words_path) as f:\r\n","        for i, word in enumerate(f.readlines(), 1):\r\n","            word_map[word.strip()] = i\r\n","    \r\n","    with open(tags_path) as f:\r\n","        for i, tag in enumerate(f.readlines(), 1):\r\n","            tag_map[tag.strip()] = i\r\n","    \r\n","    # set the O tag to 0\r\n","    tag_map['O'] = 0\r\n","    \r\n","    return word_map, tag_map\r\n"],"execution_count":69,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lDE7ktES4PMR"},"source":["# Split the data into train, val, test\r\n"]},{"cell_type":"code","metadata":{"id":"rDPZ6a4a4PQL","executionInfo":{"status":"ok","timestamp":1607732469380,"user_tz":300,"elapsed":4493,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def split_sentences(sentences_path, labels_path, ratio=0.9):\r\n","    \"\"\"\r\n","    \"\"\"\r\n","    with open(sentences_path) as f:\r\n","        sentences = f.readlines()\r\n","\r\n","    with open(labels_path) as f:\r\n","        labels = f.readlines()\r\n","\r\n","    # 90% train, 5% val and 5% test\r\n","    sen_len = len(sentences)\r\n","    train_split = int(sen_len * ratio)\r\n","    x_train = sentences[:train_split]\r\n","    y_train = labels[:train_split]\r\n","\r\n","    val_split = sen_len - train_split\r\n","    val_split = int(val_split / 2)\r\n","    val_split = train_split + val_split\r\n","    \r\n","    x_val = sentences[train_split:val_split]\r\n","    y_val = labels[train_split:val_split]\r\n","\r\n","    x_test = sentences[val_split:]\r\n","    y_test = labels[val_split:]\r\n","\r\n","    return x_train, y_train, x_val, y_val, x_test, y_test\r\n","    \r\n","\r\n"],"execution_count":70,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TU9hEIjMMn8q"},"source":["# Transform each sentence and labels to numbers\r\n"]},{"cell_type":"code","metadata":{"id":"d-6iKvdxMoFi","executionInfo":{"status":"ok","timestamp":1607732469382,"user_tz":300,"elapsed":4490,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["def transform2numbers(word_map, tag_map, sentences, tags):\r\n","    \"\"\"\r\n","    \"\"\"\r\n","    data = []\r\n","    labels = []\r\n","    for sentence, tag in zip(sentences, tags):\r\n","        # replace each token by its index\r\n","        # if it is in the word_map else\r\n","        # use the UNK token\r\n","        tokens = [word_map[token] if token in word_map else word_map['UNK'] for token in sentence.strip().split(' ')]\r\n","        label = [tag_map[token] for token in tag.strip().split(' ')]\r\n","        data.append(tokens)\r\n","        labels.append(label)\r\n","\r\n","    return data, labels, len(data)\r\n","    \r\n"],"execution_count":71,"outputs":[]},{"cell_type":"code","metadata":{"id":"mINqyoEPPhjs","executionInfo":{"status":"ok","timestamp":1607732470248,"user_tz":300,"elapsed":5351,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":["#create_vocab(df)\n","#create_tags(df)\n","#create_sentences_labels(df)\n","\n","# Dicts word to index and tag to index\n","word_map, tag_map = words_tags2index(words_path, tags_path)\n","\n","# with the sentences and labels created split the data\n","x_train, y_train, x_val, y_val, x_test, y_test = split_sentences(sentences_path, labels_path)\n","\n","# Transform the splits into numbers\n","train_data, train_labels, train_size = transform2numbers(word_map, tag_map, x_train, y_train)\n","val_data, val_labels, val_size = transform2numbers(word_map, tag_map, x_val, y_val)\n","test_data, test_labels, test_size = transform2numbers(word_map, tag_map, x_test, y_test)"],"execution_count":72,"outputs":[]},{"cell_type":"code","metadata":{"id":"7vxIMD8bShvI","executionInfo":{"status":"ok","timestamp":1607732470251,"user_tz":300,"elapsed":5352,"user":{"displayName":"Jonathan David Elejalde Gutiérrez","photoUrl":"","userId":"11854874717100226645"}}},"source":[""],"execution_count":72,"outputs":[]}]}